{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression, make_classification\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(37)\n",
    "\n",
    "def get_regression_data():\n",
    "    return make_regression(**{\n",
    "        'n_samples': 1000,\n",
    "        'n_features': 50,\n",
    "        'n_informative': 10,\n",
    "        'n_targets': 1,\n",
    "        'bias': 5.3,\n",
    "        'random_state': 37\n",
    "    })\n",
    "\n",
    "def get_classification_data():\n",
    "    return make_classification(**{\n",
    "        'n_samples': 2000,\n",
    "        'n_features': 20,\n",
    "        'n_informative': 2,\n",
    "        'n_redundant': 2,\n",
    "        'n_repeated': 0,\n",
    "        'n_classes': 2,\n",
    "        'n_clusters_per_class': 2,\n",
    "        'random_state': 37\n",
    "    })\n",
    "\n",
    "A, b = get_regression_data()\n",
    "C, d = get_classification_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate \n",
    "\n",
    "Univariate feature selection determines importance of each feature individually. This approach is accomplished through `GenericUnivariateSelect`. In a classification problem, use `chi2` or `mutual_info_classif` for the score function. Note that `chi2` requires your feature matrix to be non-negative. There are a variety of modes, but we are using the `percentile` and `k_best` modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9765500000000001\n",
      "0.977\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import GenericUnivariateSelect\n",
    "from sklearn.feature_selection import chi2, mutual_info_classif\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def get_best_indexes(scores, max_index, reverse=True):\n",
    "    tups = sorted([(i, s) for i, s in enumerate(scores)], key=lambda tup: tup[1], reverse=reverse)\n",
    "    tups = tups[:max_index]\n",
    "    return [t[0] for t in tups]\n",
    "\n",
    "def get_classification_performance(tr_index, te_index, X, y, selector):\n",
    "    X_tr, X_te = X[tr_index], X[te_index]\n",
    "    y_tr, y_te = y[tr_index], y[te_index]\n",
    "    \n",
    "    rf = RandomForestClassifier(max_depth=10, random_state=37, n_jobs=-1)\n",
    "    \n",
    "    model = Pipeline([\n",
    "        ('selector', selector),\n",
    "        ('rf', rf)\n",
    "    ])\n",
    "    \n",
    "    model.fit(X_tr, y_tr)\n",
    "    y_pr = model.predict_proba(X_te)[:, 1]\n",
    "    \n",
    "    return roc_auc_score(y_te, y_pr)\n",
    "\n",
    "p_selector = GenericUnivariateSelect(**{\n",
    "    'score_func': mutual_info_classif, \n",
    "    'mode': 'percentile', \n",
    "    'param': 15\n",
    "})\n",
    "\n",
    "k_selector = GenericUnivariateSelect(**{\n",
    "    'score_func': mutual_info_classif, \n",
    "    'mode': 'k_best', \n",
    "    'param': 2\n",
    "})\n",
    "\n",
    "tr_index, te_index = next(StratifiedKFold(n_splits=10, shuffle=True, random_state=37).split(C, d))\n",
    "\n",
    "print(get_classification_performance(tr_index, te_index, C, d, p_selector))\n",
    "print(get_classification_performance(tr_index, te_index, C, d, k_selector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When your output variable is continuous, use `f_regression` and `mutual_info_regression` for the score function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.42457694230028\n",
      "134.7419724227303\n",
      "80.69262206441475\n",
      "138.0925739351483\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import f_regression, mutual_info_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def get_regression_performance(tr_index, te_index, X, y, selector):\n",
    "    X_tr, X_te = X[tr_index], X[te_index]\n",
    "    y_tr, y_te = y[tr_index], y[te_index]\n",
    "    \n",
    "    rf = RandomForestRegressor(max_depth=10, random_state=37, n_jobs=-1)\n",
    "    \n",
    "    model = Pipeline([\n",
    "        ('selector', selector),\n",
    "        ('rf', rf)\n",
    "    ])\n",
    "    \n",
    "    model.fit(X_tr, y_tr)\n",
    "    y_pr = model.predict(X_te)\n",
    "    \n",
    "    return mean_absolute_error(y_te, y_pr)\n",
    "\n",
    "fp_selector = GenericUnivariateSelect(**{\n",
    "    'score_func': f_regression, \n",
    "    'mode': 'percentile', \n",
    "    'param': 15\n",
    "})\n",
    "\n",
    "mp_selector = GenericUnivariateSelect(**{\n",
    "    'score_func': mutual_info_regression, \n",
    "    'mode': 'percentile', \n",
    "    'param': 15\n",
    "})\n",
    "\n",
    "fk_selector = GenericUnivariateSelect(**{\n",
    "    'score_func': f_regression, \n",
    "    'mode': 'k_best', \n",
    "    'param': 2\n",
    "})\n",
    "\n",
    "mk_selector = GenericUnivariateSelect(**{\n",
    "    'score_func': mutual_info_regression, \n",
    "    'mode': 'k_best', \n",
    "    'param': 2\n",
    "})\n",
    "\n",
    "tr_index, te_index = next(KFold(n_splits=10, shuffle=True, random_state=37).split(A, b))\n",
    "\n",
    "print(get_regression_performance(tr_index, te_index, A, b, fp_selector))\n",
    "print(get_regression_performance(tr_index, te_index, A, b, fk_selector))\n",
    "print(get_regression_performance(tr_index, te_index, A, b, mp_selector))\n",
    "print(get_regression_performance(tr_index, te_index, A, b, mk_selector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "Models maybe used to select features as well through using `SelectFromModel`. The coefficients or variable importances of a model may be used to decide which features will be useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9742500000000001\n",
      "0.9742500000000001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_selector = SelectFromModel(**{\n",
    "    'estimator': LogisticRegression(n_jobs=-1),\n",
    "    'max_features': 5\n",
    "})\n",
    "\n",
    "rf_selector = SelectFromModel(**{\n",
    "    'estimator': RandomForestClassifier(max_depth=10, random_state=37, n_jobs=-1),\n",
    "    'max_features': 5\n",
    "})\n",
    "\n",
    "tr_index, te_index = next(StratifiedKFold(n_splits=10, shuffle=True, random_state=37).split(C, d))\n",
    "\n",
    "print(get_classification_performance(tr_index, te_index, C, d, lr_selector))\n",
    "print(get_classification_performance(tr_index, te_index, C, d, rf_selector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.75100069641536\n",
      "76.38248480356026\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr_selector = SelectFromModel(**{\n",
    "    'estimator': LinearRegression(n_jobs=-1),\n",
    "    'max_features': 5\n",
    "})\n",
    "\n",
    "rf_selector = SelectFromModel(**{\n",
    "    'estimator': RandomForestRegressor(max_depth=10, random_state=37, n_jobs=-1),\n",
    "    'max_features': 5\n",
    "})\n",
    "\n",
    "tr_index, te_index = next(KFold(n_splits=10, shuffle=True, random_state=37).split(A, b))\n",
    "\n",
    "print(get_regression_performance(tr_index, te_index, A, b, lr_selector))\n",
    "print(get_regression_performance(tr_index, te_index, A, b, rf_selector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential\n",
    "\n",
    "Sequential feature selection by adding (foward) or removing (backward) features is done with `SequentialFeatureSeletor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9717999999999999\n",
      "0.9739000000000001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "lr_selector = SequentialFeatureSelector(**{\n",
    "    'estimator': LogisticRegression(n_jobs=-1), \n",
    "    'n_features_to_select': 5, \n",
    "    'n_jobs': -1, \n",
    "    'scoring': 'roc_auc'})\n",
    "\n",
    "rf_selector = SequentialFeatureSelector(**{\n",
    "    'estimator': RandomForestClassifier(max_depth=10, random_state=37, n_jobs=-1), \n",
    "    'n_features_to_select': 5, \n",
    "    'n_jobs': -1, \n",
    "    'scoring': 'roc_auc'})\n",
    "\n",
    "tr_index, te_index = next(StratifiedKFold(n_splits=10, shuffle=True, random_state=37).split(C, d))\n",
    "\n",
    "print(get_classification_performance(tr_index, te_index, C, d, lr_selector))\n",
    "print(get_classification_performance(tr_index, te_index, C, d, rf_selector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.38248480356026\n",
      "76.38248480356025\n"
     ]
    }
   ],
   "source": [
    "lr_selector = SequentialFeatureSelector(**{\n",
    "    'estimator': LinearRegression(n_jobs=-1), \n",
    "    'n_features_to_select': 5, \n",
    "    'n_jobs': -1, \n",
    "    'scoring': 'neg_mean_absolute_error'})\n",
    "\n",
    "rf_selector = SequentialFeatureSelector(**{\n",
    "    'estimator': RandomForestRegressor(max_depth=10, random_state=37, n_jobs=-1), \n",
    "    'n_features_to_select': 5, \n",
    "    'n_jobs': -1, \n",
    "    'scoring': 'neg_mean_absolute_error'})\n",
    "\n",
    "tr_index, te_index = next(KFold(n_splits=10, shuffle=True, random_state=37).split(A, b))\n",
    "\n",
    "print(get_regression_performance(tr_index, te_index, A, b, lr_selector))\n",
    "print(get_regression_performance(tr_index, te_index, A, b, rf_selector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive feature elimination\n",
    "\n",
    "Recursive feature elimination with cross-validation is accomplished with `RFECV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9783\n",
      "0.9783\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "lr_selector = RFECV(LogisticRegression(n_jobs=-1), step=1, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "rf_selector = RFECV(RandomForestClassifier(max_depth=10, random_state=37, n_jobs=-1), step=1, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "tr_index, te_index = next(StratifiedKFold(n_splits=10, shuffle=True, random_state=37).split(C, d))\n",
    "\n",
    "print(get_classification_performance(tr_index, te_index, C, d, lr_selector))\n",
    "print(get_classification_performance(tr_index, te_index, C, d, rf_selector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.99916557482489\n",
      "64.42457694230026\n"
     ]
    }
   ],
   "source": [
    "lr_selector = RFECV(LinearRegression(n_jobs=-1), step=1, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "rf_selector = RFECV(RandomForestRegressor(max_depth=10, random_state=37, n_jobs=-1), step=1, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "\n",
    "tr_index, te_index = next(KFold(n_splits=10, shuffle=True, random_state=37).split(A, b))\n",
    "\n",
    "print(get_regression_performance(tr_index, te_index, A, b, lr_selector))\n",
    "print(get_regression_performance(tr_index, te_index, A, b, rf_selector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering with selection\n",
    "\n",
    "Here, we will show an example of how to do feature engineering with feature selection in a text classification problem. First, we will try to see if feature engineering (vectorization) and selection can help with classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "text = [\n",
    "    'Data Science from Scratch: First Principles with Python',\n",
    "    'Data Science for Business: What You Need to Know about Data Mining and Data-Analytic Thinking',\n",
    "    'Practical Statistics for Data Scientists',\n",
    "    'Build a Career in Data Science',\n",
    "    'Python Data Science Handbook',\n",
    "    'Storytelling with Data: A Data Visualization Guide for Business Professionals',\n",
    "    'R for Data Science: Import, Tidy, Transform, Visualize, and Model Data',\n",
    "    'Data-Driven Science and Engineering: Machine Learning, Dynamical Systems, and Control',\n",
    "    'A Hands-On Introduction to Data Science',\n",
    "    'Intro to Python for Computer Science and Data Science: Learning to Program with AI, Big Data and The Cloud',\n",
    "    'How Finance Works: The HBR Guide to Thinking Smart About the Numbers',\n",
    "    'The Intelligent Investor: The Definitive Book on Value Investing. A Book of Practical Counsel',\n",
    "    'Introduction to Finance: Markets, Investments, and Financial Management',\n",
    "    'Python for Finance: Mastering Data-Driven Finance',\n",
    "    'The Infographic Guide to Personal Finance: A Visual Reference for Everything You Need to Know',\n",
    "    'Personal Finance For Dummies',\n",
    "    'Corporate Finance For Dummies',\n",
    "    'Lords of Finance: The Bankers Who Broke the World',\n",
    "    'Real Estate Finance & Investments',\n",
    "    'Real Estate Finance and Investments Risks and Opportunities'\n",
    "]\n",
    "\n",
    "clazz = [1 for _ in range(10)] + [0 for _ in range(10)]\n",
    "\n",
    "with open('stop-words.txt', 'r') as f:\n",
    "    stop_words = set([word.strip() for word in f if len(word.strip()) > 0])\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True, stop_words=stop_words, \n",
    "                             ngram_range=(1, 2))\n",
    "selector = RFECV(RandomForestClassifier(n_estimators=20, n_jobs=-1, random_state=37), \n",
    "                 step=1, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "regressor = LogisticRegression(penalty='l2', solver='liblinear', \n",
    "                               fit_intercept=False, C=0.01, random_state=37)\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('selector', selector),\n",
    "    ('regressor', regressor)\n",
    "])\n",
    "\n",
    "pipeline.fit(text, clazz)\n",
    "y_pred = pipeline.predict(text)\n",
    "roc_auc_score(clazz, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important features (or words or phrases) are reduced to just 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data       0.043157\n",
       "finance   -0.044115\n",
       "science    0.038370\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "features_selected = sorted([(n, r) for n, r in zip(vectorizer.get_feature_names(), selector.ranking_)], \n",
    "                           key=lambda tup: tup[1])[0:selector.n_features_]\n",
    "s = pd.Series(regressor.coef_[0], index=[tup[0] for tup in features_selected])\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we do a k-fold cross validation. As can be seen below, the accuracy is pretty high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold = 00\n",
      "\tauc=1.0\n",
      "\tdata data=0.00499, data science=0.02905, finance=-0.03440, science=0.03397\n",
      "fold = 01\n",
      "\tauc=1.0\n",
      "\tdata=0.03347, data science=0.02864, finance=-0.03448, science=0.02864\n",
      "fold = 02\n",
      "\tauc=1.0\n",
      "\tdata=0.03389, finance=-0.03448, science=0.02906\n",
      "fold = 03\n",
      "\tauc=1.0\n",
      "\tdata=0.03390, finance=-0.03930, science=0.02906\n",
      "fold = 04\n",
      "\tauc=1.0\n",
      "\tdata=0.03864, finance=-0.03440, science=0.03373\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    vectorizer = CountVectorizer(binary=True, stop_words=stop_words, \n",
    "                             ngram_range=(1, 2))\n",
    "    selector = RFECV(RandomForestClassifier(n_estimators=20, n_jobs=-1, random_state=37), \n",
    "                     step=1, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "    regressor = LogisticRegression(penalty='l2', solver='liblinear', \n",
    "                                   fit_intercept=False, C=0.01, random_state=37)\n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer', vectorizer),\n",
    "        ('selector', selector),\n",
    "        ('regressor', regressor)\n",
    "    ])\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "for fold, (tr, te) in enumerate(StratifiedKFold(n_splits=5, shuffle=True, random_state=37).split(text, clazz)):\n",
    "    X = np.array(text)\n",
    "    y = np.array(clazz)\n",
    "    \n",
    "    X_tr, X_te = X[tr], X[te]\n",
    "    y_tr, y_te = y[tr], y[te]\n",
    "    \n",
    "    model = get_model()\n",
    "    model.fit(X_tr, y_tr)\n",
    "    y_pred = model.predict_proba(X_te)[:, 1]\n",
    "    \n",
    "    score = roc_auc_score(y_te, y_pred)\n",
    "    \n",
    "    vectorizer = model['vectorizer']\n",
    "    selector = model['selector']\n",
    "    \n",
    "    features = vectorizer.get_feature_names()\n",
    "    rankings = selector.ranking_\n",
    "    \n",
    "    features_selected = sorted([(n, r) for n, r in zip(features, rankings)], \n",
    "                               key=lambda tup: tup[1])[0:selector.n_features_]\n",
    "    features_selected = [tup[0] for tup in features_selected]\n",
    "    \n",
    "    regressor = model['regressor']\n",
    "    coefs = regressor.coef_[0]\n",
    "    \n",
    "    features_selected = pd.Series(coefs, index=features_selected)\n",
    "    features_selected = ', '.join([f'{i}={v:.5f}' for i, v in zip(features_selected.index, features_selected.values)])\n",
    "    \n",
    "    print(f'fold = {fold:02}')\n",
    "    print(f'\\tauc={score}')\n",
    "    print(f'\\t{features_selected}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
