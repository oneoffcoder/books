{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression, make_classification\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(37)\n",
    "\n",
    "def get_regression_data():\n",
    "    return make_regression(**{\n",
    "        'n_samples': 1000,\n",
    "        'n_features': 50,\n",
    "        'n_informative': 10,\n",
    "        'n_targets': 1,\n",
    "        'bias': 5.3,\n",
    "        'random_state': 37\n",
    "    })\n",
    "\n",
    "def get_classification_data():\n",
    "    return make_classification(**{\n",
    "        'n_samples': 2000,\n",
    "        'n_features': 20,\n",
    "        'n_informative': 2,\n",
    "        'n_redundant': 2,\n",
    "        'n_repeated': 0,\n",
    "        'n_classes': 2,\n",
    "        'n_clusters_per_class': 2,\n",
    "        'random_state': 37\n",
    "    })\n",
    "\n",
    "A, b = get_regression_data()\n",
    "C, d = get_classification_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate \n",
    "\n",
    "Univariate feature selection determines importance of each feature individually. This approach is accomplished through `GenericUnivariateSelect`. In a classification problem, use `chi2` or `mutual_info_classif` for the score function. Note that `chi2` requires your feature matrix to be non-negative. There are a variety of modes, but we are using the `percentile` and `k_best` modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9765500000000001\n",
      "0.977\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import GenericUnivariateSelect\n",
    "from sklearn.feature_selection import chi2, mutual_info_classif\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def get_best_indexes(scores, max_index, reverse=True):\n",
    "    tups = sorted([(i, s) for i, s in enumerate(scores)], key=lambda tup: tup[1], reverse=reverse)\n",
    "    tups = tups[:max_index]\n",
    "    return [t[0] for t in tups]\n",
    "\n",
    "def get_classification_performance(tr_index, te_index, X, y, selector):\n",
    "    X_tr, X_te = X[tr_index], X[te_index]\n",
    "    y_tr, y_te = y[tr_index], y[te_index]\n",
    "    \n",
    "    rf = RandomForestClassifier(max_depth=10, random_state=37, n_jobs=-1)\n",
    "    \n",
    "    model = Pipeline([\n",
    "        ('selector', selector),\n",
    "        ('rf', rf)\n",
    "    ])\n",
    "    \n",
    "    model.fit(X_tr, y_tr)\n",
    "    y_pr = model.predict_proba(X_te)[:, 1]\n",
    "    \n",
    "    return roc_auc_score(y_te, y_pr)\n",
    "\n",
    "p_selector = GenericUnivariateSelect(**{\n",
    "    'score_func': mutual_info_classif, \n",
    "    'mode': 'percentile', \n",
    "    'param': 15\n",
    "})\n",
    "\n",
    "k_selector = GenericUnivariateSelect(**{\n",
    "    'score_func': mutual_info_classif, \n",
    "    'mode': 'k_best', \n",
    "    'param': 2\n",
    "})\n",
    "\n",
    "tr_index, te_index = next(StratifiedKFold(n_splits=10, shuffle=True, random_state=37).split(C, d))\n",
    "\n",
    "print(get_classification_performance(tr_index, te_index, C, d, p_selector))\n",
    "print(get_classification_performance(tr_index, te_index, C, d, k_selector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When your output variable is continuous, use `f_regression` and `mutual_info_regression` for the score function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.62026528579264\n",
      "134.81849592816624\n",
      "80.6040460679759\n",
      "138.10921666118406\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import f_regression, mutual_info_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def get_regression_performance(tr_index, te_index, X, y, selector):\n",
    "    X_tr, X_te = X[tr_index], X[te_index]\n",
    "    y_tr, y_te = y[tr_index], y[te_index]\n",
    "    \n",
    "    rf = RandomForestRegressor(max_depth=10, random_state=37, n_jobs=-1)\n",
    "    \n",
    "    model = Pipeline([\n",
    "        ('selector', selector),\n",
    "        ('rf', rf)\n",
    "    ])\n",
    "    \n",
    "    model.fit(X_tr, y_tr)\n",
    "    y_pr = model.predict(X_te)\n",
    "    \n",
    "    return mean_absolute_error(y_te, y_pr)\n",
    "\n",
    "fp_selector = GenericUnivariateSelect(**{\n",
    "    'score_func': f_regression, \n",
    "    'mode': 'percentile', \n",
    "    'param': 15\n",
    "})\n",
    "\n",
    "mp_selector = GenericUnivariateSelect(**{\n",
    "    'score_func': mutual_info_regression, \n",
    "    'mode': 'percentile', \n",
    "    'param': 15\n",
    "})\n",
    "\n",
    "fk_selector = GenericUnivariateSelect(**{\n",
    "    'score_func': f_regression, \n",
    "    'mode': 'k_best', \n",
    "    'param': 2\n",
    "})\n",
    "\n",
    "mk_selector = GenericUnivariateSelect(**{\n",
    "    'score_func': mutual_info_regression, \n",
    "    'mode': 'k_best', \n",
    "    'param': 2\n",
    "})\n",
    "\n",
    "tr_index, te_index = next(KFold(n_splits=10, shuffle=True, random_state=37).split(A, b))\n",
    "\n",
    "print(get_regression_performance(tr_index, te_index, A, b, fp_selector))\n",
    "print(get_regression_performance(tr_index, te_index, A, b, fk_selector))\n",
    "print(get_regression_performance(tr_index, te_index, A, b, mp_selector))\n",
    "print(get_regression_performance(tr_index, te_index, A, b, mk_selector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "Models maybe used to select features as well through using `SelectFromModel`. The coefficients or variable importances of a model may be used to decide which features will be useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9742500000000001\n",
      "0.9742500000000001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_selector = SelectFromModel(**{\n",
    "    'estimator': LogisticRegression(n_jobs=-1),\n",
    "    'max_features': 5\n",
    "})\n",
    "\n",
    "rf_selector = SelectFromModel(**{\n",
    "    'estimator': RandomForestClassifier(max_depth=10, random_state=37, n_jobs=-1),\n",
    "    'max_features': 5\n",
    "})\n",
    "\n",
    "tr_index, te_index = next(StratifiedKFold(n_splits=10, shuffle=True, random_state=37).split(C, d))\n",
    "\n",
    "print(get_classification_performance(tr_index, te_index, C, d, lr_selector))\n",
    "print(get_classification_performance(tr_index, te_index, C, d, rf_selector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.86886242490726\n",
      "76.19174124554559\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr_selector = SelectFromModel(**{\n",
    "    'estimator': LinearRegression(n_jobs=-1),\n",
    "    'max_features': 5\n",
    "})\n",
    "\n",
    "rf_selector = SelectFromModel(**{\n",
    "    'estimator': RandomForestRegressor(max_depth=10, random_state=37, n_jobs=-1),\n",
    "    'max_features': 5\n",
    "})\n",
    "\n",
    "tr_index, te_index = next(KFold(n_splits=10, shuffle=True, random_state=37).split(A, b))\n",
    "\n",
    "print(get_regression_performance(tr_index, te_index, A, b, lr_selector))\n",
    "print(get_regression_performance(tr_index, te_index, A, b, rf_selector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential\n",
    "\n",
    "Sequential feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SequentialFeatureSelector' from 'sklearn.feature_selection' (C:\\Users\\jeev\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-e9ec6ca07ff3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequentialFeatureSelector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mSequentialFeatureSelector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features_to_select\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'neg_mean_absolute_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'SequentialFeatureSelector' from 'sklearn.feature_selection' (C:\\Users\\jeev\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "SequentialFeatureSelector(LogisticRegression(n_jobs=-1), n_features_to_select=5, n_jobs=-1, scoring='neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
