{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask\n",
    "\n",
    "`Dask` is a parallel processing API that has analogous data structures to `numpy` arrays, `pandas` dataframes and `Spark` RDDs. In the cases of arrays and dataframes, unlike their original data structures in numpy and pandas, the `Dask` versions are distributed and operations are parallelized. The `bag` data structure of dask is the analog to a Spark RDD, and both are distributed data structures. These distributed data structures are called `high-level collections` in Dask.\n",
    "\n",
    "| Dask Analog | Existing Data Structures |\n",
    "| --- | ----------- |\n",
    "| Array | Numpy Array |\n",
    "| DataFrame | Pandas DataFrame |\n",
    "| Bag | Spark RDD |\n",
    "\n",
    "Dask seems to take the best of what's fundamental and available to the Python data science toolset and unifies them into a single framework. If you have ever worked with [Apache Spark](https://spark.apache.org/) and seen its web interface, Dask has a similar dashboard (runs on port `8787` by default). Like Spark, Dask is also built on the concept of a `driver` submitting jobs (centered on a distributed data structure) to a `cluster` of workers. Here's a more in depth [comparison and contrast](https://docs.dask.org/en/latest/spark.html) of Dask and Spark. Let's take glance at dask and see how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster and client\n",
    "\n",
    "In Dask, the `Client` represents the `driver` and the `LocalCluster` represents the `cluster`. The driver controls the manipulation of distributed data through submission of jobs to the cluster. The `LocalCluster` is not a real cluster of separate, physical worker nodes, but it mimics one and is useful for local development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.status.idle": "2020-12-16T08:21:53.551293Z",
     "shell.execute_reply": "2020-12-16T08:21:53.549285Z",
     "shell.execute_reply.started": "2020-12-16T08:21:49.187293Z"
    }
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "params = {\n",
    "    'n_workers': 4,\n",
    "    'threads_per_worker': 2,\n",
    "    'dashboard_address': '8787'\n",
    "}\n",
    "cluster = LocalCluster(**params)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-16T08:21:53.554291Z",
     "iopub.status.busy": "2020-12-16T08:21:53.553293Z",
     "iopub.status.idle": "2020-12-16T08:21:53.581289Z",
     "shell.execute_reply": "2020-12-16T08:21:53.579293Z",
     "shell.execute_reply.started": "2020-12-16T08:21:53.554291Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:60086</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>25.76 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:60086' processes=4 threads=8, memory=25.76 GB>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame\n",
    "\n",
    "A Dask [DataFrame](https://docs.dask.org/en/latest/dataframe-api.html) behaves nearly identical to a pandas one. There's a couple of ways to create a Dask DataFrame (e.g. reading from files), but, here, we create a Pandas one and use the `from_pandas()` function to convert the Pandas DataFrame to a Dask one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-16T08:21:53.584298Z",
     "iopub.status.busy": "2020-12-16T08:21:53.584298Z",
     "iopub.status.idle": "2020-12-16T08:21:54.386288Z",
     "shell.execute_reply": "2020-12-16T08:21:54.384283Z",
     "shell.execute_reply.started": "2020-12-16T08:21:53.584298Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_regression, make_classification\n",
    "\n",
    "np.random.seed(37)\n",
    "random.seed(37)\n",
    "\n",
    "def to_pdf(X, y):\n",
    "    A = pd.DataFrame(X, columns=[f'x{i}' for i in range(X.shape[1])])\n",
    "    b = pd.DataFrame(pd.Series(y, name='y'))\n",
    "    return A, b\n",
    "\n",
    "def to_seq(X, y):\n",
    "    A = [{c: r[c] for c in X.columns} for _, r in X.iterrows()]\n",
    "    b = [{c: r[c] for c in y.columns} for _, r in y.iterrows()]\n",
    "    return A, b\n",
    "\n",
    "def get_regression(n_samples=2000):\n",
    "    X, y = make_regression(**{\n",
    "        'n_samples': n_samples,\n",
    "        'n_features': 10,\n",
    "        'n_informative': 5,\n",
    "        'n_targets': 1,\n",
    "        'bias': 5.3,\n",
    "        'random_state': 37\n",
    "    })\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def get_classification(n_samples=2000):\n",
    "    X, y = make_classification(**{\n",
    "        'n_samples': n_samples,\n",
    "        'n_features': 10,\n",
    "        'n_informative': 5,\n",
    "        'n_redundant': 2,\n",
    "        'n_repeated': 0,\n",
    "        'n_classes': 2,\n",
    "        'n_clusters_per_class': 2,\n",
    "        'random_state': 37\n",
    "    })\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X, y = get_regression()\n",
    "X, y = to_pdf(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with Spark, we have to worry about the number of partitions in Dask as well. For now, we will specify 100 arbitrarily. Partitioning the data is always important as it impacts the distribution of computations and memory usage. When specifying `npartitions`, this is the number of partitions you want, but you could end up with less partitions than requested. There's another option, `chunksize`, that specifies the number of records per partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-16T08:21:54.389290Z",
     "iopub.status.busy": "2020-12-16T08:21:54.389290Z",
     "iopub.status.idle": "2020-12-16T08:21:54.765284Z",
     "shell.execute_reply": "2020-12-16T08:21:54.764288Z",
     "shell.execute_reply.started": "2020-12-16T08:21:54.389290Z"
    }
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "X = dd.from_pandas(X, npartitions=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When attempting to get the string representation of a Dask DataFrame, notice how the actual data is not displayed, but, rather, just some metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-16T08:21:54.767287Z",
     "iopub.status.busy": "2020-12-16T08:21:54.767287Z",
     "iopub.status.idle": "2020-12-16T08:21:54.826286Z",
     "shell.execute_reply": "2020-12-16T08:21:54.812285Z",
     "shell.execute_reply.started": "2020-12-16T08:21:54.767287Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=100</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: from_pandas, 100 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                      x0       x1       x2       x3       x4       x5       x6       x7       x8       x9\n",
       "npartitions=100                                                                                          \n",
       "0                float64  float64  float64  float64  float64  float64  float64  float64  float64  float64\n",
       "20                   ...      ...      ...      ...      ...      ...      ...      ...      ...      ...\n",
       "...                  ...      ...      ...      ...      ...      ...      ...      ...      ...      ...\n",
       "1980                 ...      ...      ...      ...      ...      ...      ...      ...      ...      ...\n",
       "1999                 ...      ...      ...      ...      ...      ...      ...      ...      ...      ...\n",
       "Dask Name: from_pandas, 100 tasks"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the data types of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-16T08:21:54.828288Z",
     "iopub.status.busy": "2020-12-16T08:21:54.828288Z",
     "iopub.status.idle": "2020-12-16T08:21:54.852286Z",
     "shell.execute_reply": "2020-12-16T08:21:54.842280Z",
     "shell.execute_reply.started": "2020-12-16T08:21:54.828288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x0    float64\n",
       "x1    float64\n",
       "x2    float64\n",
       "x3    float64\n",
       "x4    float64\n",
       "x5    float64\n",
       "x6    float64\n",
       "x7    float64\n",
       "x8    float64\n",
       "x9    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can invoke statistical functions like `sum()`, `mean()` and `std()` on the DataFrame. However, if we want the results to come back to the client, we need to issue `compute()`. This pattern of functions mimics Spark's `transformation` vs `action` functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-16T08:21:54.854289Z",
     "iopub.status.busy": "2020-12-16T08:21:54.854289Z",
     "iopub.status.idle": "2020-12-16T08:21:56.415286Z",
     "shell.execute_reply": "2020-12-16T08:21:56.413289Z",
     "shell.execute_reply.started": "2020-12-16T08:21:54.854289Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x0     8.309788\n",
       "x1    -7.660057\n",
       "x2    -1.745221\n",
       "x3    -8.071736\n",
       "x4   -25.665312\n",
       "x5    70.528265\n",
       "x6   -18.859073\n",
       "x7   -15.195804\n",
       "x8   -46.170896\n",
       "x9   -29.945949\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-16T08:21:56.417290Z",
     "iopub.status.busy": "2020-12-16T08:21:56.416289Z",
     "iopub.status.idle": "2020-12-16T08:21:57.273289Z",
     "shell.execute_reply": "2020-12-16T08:21:57.271288Z",
     "shell.execute_reply.started": "2020-12-16T08:21:56.417290Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x0    0.004155\n",
       "x1   -0.003830\n",
       "x2   -0.000873\n",
       "x3   -0.004036\n",
       "x4   -0.012833\n",
       "x5    0.035264\n",
       "x6   -0.009430\n",
       "x7   -0.007598\n",
       "x8   -0.023085\n",
       "x9   -0.014973\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-16T08:21:57.277293Z",
     "iopub.status.busy": "2020-12-16T08:21:57.277293Z",
     "iopub.status.idle": "2020-12-16T08:21:57.796286Z",
     "shell.execute_reply": "2020-12-16T08:21:57.794288Z",
     "shell.execute_reply.started": "2020-12-16T08:21:57.277293Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x0    0.984805\n",
       "x1    1.004140\n",
       "x2    1.014151\n",
       "x3    1.016681\n",
       "x4    0.982926\n",
       "x5    0.981421\n",
       "x6    0.986267\n",
       "x7    1.025351\n",
       "x8    1.019498\n",
       "x9    0.986211\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.std().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag\n",
    "\n",
    "Dask [Bags](https://docs.dask.org/en/latest/bag-api.html) are like Spark RDDs. You get similar functions to RDDs with Bags like `map()`, `filter()` and `reduce()`. The `reduceByKey()` in Spark RDDs is `foldby()` in Dask Bags. The `reduce()` in Spark RDDs is `fold()` in Dask Bags. \n",
    "\n",
    "Below, to create a Dask Bag, we use `from_sequence()` where our collection has dictionary elements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-16T08:21:57.800288Z",
     "iopub.status.busy": "2020-12-16T08:21:57.799291Z",
     "iopub.status.idle": "2020-12-16T08:21:58.709291Z",
     "shell.execute_reply": "2020-12-16T08:21:58.708289Z",
     "shell.execute_reply.started": "2020-12-16T08:21:57.800288Z"
    }
   },
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "\n",
    "X, y = get_regression()\n",
    "X, y = to_pdf(X, y)\n",
    "X, y = to_seq(X, y)\n",
    "X = db.from_sequence(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of map and reduce by key. In the map operation, we map a dictionary element to a tuple, where the first element is a boolean indicating if the integer representation of the `x0` field is even, and the second element is the value of `x0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-16T08:21:58.711289Z",
     "iopub.status.busy": "2020-12-16T08:21:58.710289Z",
     "iopub.status.idle": "2020-12-16T08:21:59.422290Z",
     "shell.execute_reply": "2020-12-16T08:21:59.420289Z",
     "shell.execute_reply.started": "2020-12-16T08:21:58.711289Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(True, (True, -7.959470449062381)), (False, (False, 16.269258267363348))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X.map(lambda r: (int(r['x0']) % 2 == 0, r['x0']))\n",
    " .foldby(lambda tup: tup[0], lambda a, b: (a[0], a[1] + b[1]))\n",
    " .compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we filter for even numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-16T08:21:59.425286Z",
     "iopub.status.busy": "2020-12-16T08:21:59.424289Z",
     "iopub.status.idle": "2020-12-16T08:22:00.038295Z",
     "shell.execute_reply": "2020-12-16T08:22:00.037295Z",
     "shell.execute_reply.started": "2020-12-16T08:21:59.425286Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(True, (True, -7.959470449062381))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X.map(lambda r: (int(r['x0']) % 2 == 0, r['x0']))\n",
    " .filter(lambda tup: tup[0])\n",
    " .foldby(lambda tup: tup[0], lambda a, b: (a[0], a[1] + b[1]))\n",
    " .compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exampke, we filter for odd numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-16T08:22:00.040295Z",
     "iopub.status.busy": "2020-12-16T08:22:00.040295Z",
     "iopub.status.idle": "2020-12-16T08:22:00.623289Z",
     "shell.execute_reply": "2020-12-16T08:22:00.621291Z",
     "shell.execute_reply.started": "2020-12-16T08:22:00.040295Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(False, (False, 16.269258267363348))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X.map(lambda r: (int(r['x0']) % 2 == 0, r['x0']))\n",
    " .filter(lambda tup: not tup[0])\n",
    " .foldby(lambda tup: tup[0], lambda a, b: (a[0], a[1] + b[1]))\n",
    " .compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would be your classic map, filter and reduce example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-16T08:22:00.625291Z",
     "iopub.status.busy": "2020-12-16T08:22:00.625291Z",
     "iopub.status.idle": "2020-12-16T08:22:01.256289Z",
     "shell.execute_reply": "2020-12-16T08:22:01.254288Z",
     "shell.execute_reply.started": "2020-12-16T08:22:00.625291Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.959470449062381"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X.map(lambda r: r['x0'])\n",
    " .filter(lambda x0: int(x0) % 2 == 0)\n",
    " .fold(lambda a, b: a + b)\n",
    " .compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "\n",
    "Dask has a machine learning package [dask-ml](https://anaconda.org/conda-forge/dask-ml). It's very cool the API has a `make_classification()` method just like with `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-16T08:22:01.257287Z",
     "iopub.status.busy": "2020-12-16T08:22:01.257287Z",
     "iopub.status.idle": "2020-12-16T08:22:01.413289Z",
     "shell.execute_reply": "2020-12-16T08:22:01.412289Z",
     "shell.execute_reply.started": "2020-12-16T08:22:01.257287Z"
    }
   },
   "outputs": [],
   "source": [
    "from dask_ml.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(chunks=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-16T08:22:01.415290Z",
     "iopub.status.busy": "2020-12-16T08:22:01.415290Z",
     "iopub.status.idle": "2020-12-16T08:22:01.429288Z",
     "shell.execute_reply": "2020-12-16T08:22:01.428289Z",
     "shell.execute_reply.started": "2020-12-16T08:22:01.415290Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>\n",
       "  <thead>\n",
       "    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 16.00 kB </td> <td> 8.00 kB </td></tr>\n",
       "    <tr><th> Shape </th><td> (100, 20) </td> <td> (50, 20) </td></tr>\n",
       "    <tr><th> Count </th><td> 2 Tasks </td><td> 2 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> float64 </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</td>\n",
       "<td>\n",
       "<svg width=\"92\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"42\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"60\" x2=\"42\" y2=\"60\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"42\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"42\" y1=\"0\" x2=\"42\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 42.00989029700999,0.0 42.00989029700999,120.0 0.0,120.0\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"21.004945\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >20</text>\n",
       "  <text x=\"62.009890\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,62.009890,60.000000)\">100</text>\n",
       "</svg>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<normal, shape=(100, 20), dtype=float64, chunksize=(50, 20), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-16T08:22:01.432291Z",
     "iopub.status.busy": "2020-12-16T08:22:01.432291Z",
     "iopub.status.idle": "2020-12-16T08:22:01.446295Z",
     "shell.execute_reply": "2020-12-16T08:22:01.444293Z",
     "shell.execute_reply.started": "2020-12-16T08:22:01.432291Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>\n",
       "  <thead>\n",
       "    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 400 B </td> <td> 200 B </td></tr>\n",
       "    <tr><th> Shape </th><td> (100,) </td> <td> (50,) </td></tr>\n",
       "    <tr><th> Count </th><td> 25 Tasks </td><td> 2 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> int32 </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</td>\n",
       "<td>\n",
       "<svg width=\"170\" height=\"75\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"120\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"25\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"60\" y1=\"0\" x2=\"60\" y2=\"25\" />\n",
       "  <line x1=\"120\" y1=\"0\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 120.0,0.0 120.0,25.412616514582485 0.0,25.412616514582485\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"60.000000\" y=\"45.412617\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >100</text>\n",
       "  <text x=\"140.000000\" y=\"12.706308\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,140.000000,12.706308)\">1</text>\n",
       "</svg>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<astype, shape=(100,), dtype=int32, chunksize=(50,), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we perform logistic regression with `L2` penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-16T08:22:01.448295Z",
     "iopub.status.busy": "2020-12-16T08:22:01.447293Z",
     "iopub.status.idle": "2020-12-16T08:22:13.754291Z",
     "shell.execute_reply": "2020-12-16T08:22:13.752289Z",
     "shell.execute_reply.started": "2020-12-16T08:22:01.448295Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('intercept', 0.031604621536995815),\n",
       " (0, -0.03977702121551344),\n",
       " (1, 0.02738286821465915),\n",
       " (2, 0.03905960541075271),\n",
       " (3, 0.04244144014184735),\n",
       " (4, -0.024966699833507272),\n",
       " (5, 0.041832909297795436),\n",
       " (6, -0.06986791411991944),\n",
       " (7, -0.003705773110285104),\n",
       " (8, 0.08000742624249171),\n",
       " (9, -0.030103932549586015),\n",
       " (10, 0.02337067990625838),\n",
       " (11, -0.030984617662260194),\n",
       " (12, -0.015829171537806927),\n",
       " (13, -0.05937940996774958),\n",
       " (14, 0.012897158663833902),\n",
       " (15, 0.003511501618180998),\n",
       " (16, -0.007073093340879795),\n",
       " (17, -0.033397647822395786),\n",
       " (18, -0.11026077158815495),\n",
       " (19, 0.022812162700462206)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from dask_ml.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(penalty='l2', C=0.01, random_state=37, solver='admm')\n",
    "model.fit(X, y)\n",
    "[('intercept', model.intercept_)] + [(f, c) for f, c in zip(range(len(model.coef_)), model.coef_)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we peform logistic regression with `L1` penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-16T08:22:13.755285Z",
     "iopub.status.busy": "2020-12-16T08:22:13.755285Z",
     "iopub.status.idle": "2020-12-16T08:22:19.465290Z",
     "shell.execute_reply": "2020-12-16T08:22:19.455288Z",
     "shell.execute_reply.started": "2020-12-16T08:22:13.755285Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.67 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('intercept', 0.05775287110215685),\n",
       " (0, -0.19003640661111515),\n",
       " (1, 0.03490567942649925),\n",
       " (2, 0.06896348591496251),\n",
       " (3, 0.12080581330088788),\n",
       " (4, -0.1449561951791722),\n",
       " (5, 0.1821003499095371),\n",
       " (6, -0.377129037444486),\n",
       " (7, 0.0),\n",
       " (8, 0.30657389885412994),\n",
       " (9, -0.15861731667477016),\n",
       " (10, 0.003624785705384879),\n",
       " (11, -0.05489291271773286),\n",
       " (12, 0.0),\n",
       " (13, -0.1865770525965461),\n",
       " (14, 0.0),\n",
       " (15, 0.0),\n",
       " (16, 0.0),\n",
       " (17, -0.08624180716536158),\n",
       " (18, -0.5024166851150068),\n",
       " (19, 0.06449899347591809)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = LogisticRegression(penalty='l1', C=0.5, random_state=37, solver='admm')\n",
    "model.fit(X, y)\n",
    "[('intercept', model.intercept_)] + [(f, c) for f, c in zip(range(len(model.coef_)), model.coef_)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
