{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([1.0, 2.0, 3.0])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "Vectors.dense([1.0, 2.0, 3.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(5, {0: 1.0, 2: 3.8, 4: 8.8})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vectors.sparse(5, [(0, 1.0), (2, 3.8), (4, 8.8)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix and DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(**{\n",
    "    'n_samples': 2000,\n",
    "    'n_features': 4,\n",
    "    'n_informative': 4,\n",
    "    'n_redundant': 0,\n",
    "    'n_repeated': 0,\n",
    "    'n_classes': 2,\n",
    "    'n_clusters_per_class': 2,\n",
    "    'random_state': 37\n",
    "})\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(Vectors.dense(X[r,:].tolist()),) for r in range(X.shape[0])], \n",
    "    ['features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseMatrix([[ 1.        ,  0.19671305, -0.07915219, -0.3243779 ],\n",
      "             [ 0.19671305,  1.        ,  0.26944672,  0.00364392],\n",
      "             [-0.07915219,  0.26944672,  1.        , -0.28759495],\n",
      "             [-0.3243779 ,  0.00364392, -0.28759495,  1.        ]])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "print(str(Correlation.corr(df, 'features').head()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spearman correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseMatrix([[ 1.        ,  0.21295424, -0.11234501, -0.32188153],\n",
      "             [ 0.21295424,  1.        ,  0.27054041, -0.07225098],\n",
      "             [-0.11234501,  0.27054041,  1.        , -0.28120594],\n",
      "             [-0.32188153, -0.07225098, -0.28120594,  1.        ]])\n"
     ]
    }
   ],
   "source": [
    "print(str(Correlation.corr(df, 'features', 'spearman').head()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-square test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(**{\n",
    "    'n_samples': 100,\n",
    "    'n_features': 2,\n",
    "    'n_informative': 2,\n",
    "    'n_redundant': 0,\n",
    "    'n_repeated': 0,\n",
    "    'n_classes': 2,\n",
    "    'n_clusters_per_class': 2,\n",
    "    'random_state': 37\n",
    "})\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(float(y[r]), Vectors.dense(X[r,:].tolist())) for r in range(X.shape[0])], \n",
    "    ['label', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4529585113209542,0.4529585113209542] : p-values\n",
      "[99, 99] : dof\n",
      "[100.0,100.0] : statistics\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.stat import ChiSquareTest\n",
    "\n",
    "r = ChiSquareTest.test(df, 'features', 'label').head()\n",
    "print(f'{r.pValues} : p-values')\n",
    "print(f'{r.degreesOfFreedom} : dof')\n",
    "print(f'{r.statistics} : statistics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------+\n",
      "|aggregate_metrics(features, 1.0)                                                     |\n",
      "+-------------------------------------------------------------------------------------+\n",
      "|[[-0.0648518109322975,0.027164400904008706], [1.7191301187548882,1.6490793943014903]]|\n",
      "+-------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.stat import Summarizer\n",
    "\n",
    "s = Summarizer.metrics('mean', 'variance')\n",
    "\n",
    "df.select(s.summary(df.features)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min and max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------+\n",
      "|aggregate_metrics(features, 1.0)                                               |\n",
      "+-------------------------------------------------------------------------------+\n",
      "|[[-3.229583117714798,-2.70686498114644], [2.653016794529104,2.679714287856398]]|\n",
      "+-------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = Summarizer.metrics('min', 'max')\n",
    "\n",
    "df.select(s.summary(df.features)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|aggregate_metrics(features, 1.0)|\n",
      "+--------------------------------+\n",
      "|[100]                           |\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = Summarizer.metrics('count')\n",
    "\n",
    "df.select(s.summary(df.features)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+-----+----------+\n",
      "|probability                               |label|prediction|\n",
      "+------------------------------------------+-----+----------+\n",
      "|[0.8680409101265395,0.13195908987346056]  |0.0  |0.0       |\n",
      "|[0.08030517436073896,0.919694825639261]   |0.0  |1.0       |\n",
      "|[0.0385764052721843,0.9614235947278156]   |1.0  |1.0       |\n",
      "|[0.23941846958850424,0.7605815304114959]  |1.0  |1.0       |\n",
      "|[0.9486667828637316,0.0513332171362683]   |0.0  |0.0       |\n",
      "|[0.24662939567538006,0.7533706043246199]  |0.0  |1.0       |\n",
      "|[0.10000853755198902,0.8999914624480111]  |1.0  |1.0       |\n",
      "|[0.05211571635853627,0.9478842836414636]  |1.0  |1.0       |\n",
      "|[0.8505912777705403,0.14940872222945964]  |0.0  |0.0       |\n",
      "|[0.21719054428407597,0.7828094557159241]  |1.0  |1.0       |\n",
      "|[0.05311183455823478,0.9468881654417654]  |1.0  |1.0       |\n",
      "|[0.9482695764129414,0.05173042358705855]  |0.0  |0.0       |\n",
      "|[0.5078594474186815,0.4921405525813185]   |1.0  |0.0       |\n",
      "|[0.030506033909821213,0.9694939660901789] |1.0  |1.0       |\n",
      "|[0.18788723612617628,0.8121127638738237]  |1.0  |1.0       |\n",
      "|[0.2723894435175389,0.7276105564824611]   |1.0  |1.0       |\n",
      "|[0.0028093683234259397,0.9971906316765742]|1.0  |1.0       |\n",
      "|[0.8290838868182774,0.17091611318172253]  |0.0  |0.0       |\n",
      "|[0.8706755894225549,0.12932441057744518]  |0.0  |0.0       |\n",
      "|[0.08386681998270544,0.9161331800172946]  |1.0  |1.0       |\n",
      "+------------------------------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol='features', outputCol='scaledFeatures',\n",
    "                        withStd=True, withMean=False)\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "pipeline = Pipeline(stages=[scaler, lr])\n",
    "\n",
    "model = pipeline.fit(df)\n",
    "\n",
    "y_preds = model.transform(df)\n",
    "y_preds.select('probability', 'label', 'prediction').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extractors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------------------------------------------------------------------------------------+\n",
      "|label|features                                                                                          |\n",
      "+-----+--------------------------------------------------------------------------------------------------+\n",
      "|0.0  |(10,[4,5,7,8],[0.8472978603872037,0.0,0.6729444732424258,0.8472978603872037])                     |\n",
      "|0.0  |(10,[5,7],[0.0,0.3364722366212129])                                                               |\n",
      "|0.0  |(10,[1,5,7,9],[0.8472978603872037,0.0,0.3364722366212129,0.3364722366212129])                     |\n",
      "|1.0  |(10,[4,5,7,8,9],[0.8472978603872037,0.0,0.3364722366212129,0.8472978603872037,0.3364722366212129])|\n",
      "|1.0  |(10,[5,9],[0.0,0.3364722366212129])                                                               |\n",
      "|1.0  |(10,[1,5,9],[0.8472978603872037,0.0,0.6729444732424258])                                          |\n",
      "+-----+--------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "raw_df = spark.createDataFrame([\n",
    "    (0.0, 'How to program in Java'),\n",
    "    (0.0, 'Java recipies'),\n",
    "    (0.0, 'Learn Java in 24 hours'),\n",
    "    (1.0, 'How to program in Python'),\n",
    "    (1.0, 'Python recipies'),\n",
    "    (1.0, 'Learn Python in 24 hours')\n",
    "], ['label', 'title'])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='title', outputCol='words')\n",
    "hashing = HashingTF(inputCol='words', outputCol='raw_features', numFeatures=10)\n",
    "idf = IDF(inputCol='raw_features', outputCol='features')\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hashing, idf])\n",
    "model = pipeline.fit(raw_df)\n",
    "\n",
    "rescale_df = model.transform(raw_df)\n",
    "rescale_df.select('label', 'features').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How', 'to', 'program', 'in', 'Java'] => [0.016671489179134368,-0.07890784069895745,-0.061020128056406976]\n",
      "['Java', 'recipies'] => [-0.00206630676984787,-0.133316308259964,0.0072393231093883514]\n",
      "['Learn', 'Java', 'in', '24', 'hours'] => [-0.0064692735671997076,-0.0450890451669693,-0.044838495552539825]\n",
      "['How', 'to', 'program', 'in', 'Python'] => [0.006694729626178742,-0.02815251871943474,-0.06902075484395027]\n",
      "['Python', 'recipies'] => [-0.02700820565223694,-0.0064280033111572266,-0.01276224385946989]\n",
      "['Learn', 'Python', 'in', '24', 'hours'] => [-0.016446033120155336,0.005666276812553406,-0.052839122340083124]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "raw_df = spark.createDataFrame([\n",
    "    ('How to program in Java'.split(' '),),\n",
    "    ('Java recipies'.split(' '),),\n",
    "    ('Learn Java in 24 hours'.split(' '),),\n",
    "    ('How to program in Python'.split(' '),),\n",
    "    ('Python recipies'.split(' '),),\n",
    "    ('Learn Python in 24 hours'.split(' '),)\n",
    "], ['text'])\n",
    "\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol='text', outputCol='result')\n",
    "model = word2Vec.fit(raw_df)\n",
    "\n",
    "result = model.transform(raw_df)\n",
    "for text, vector in result.collect():\n",
    "    print(f'{text} => {vector}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+-------------------------+\n",
      "|id |words                  |features                 |\n",
      "+---+-----------------------+-------------------------+\n",
      "|0  |[at, bat, cat]         |(3,[0,1,2],[1.0,1.0,1.0])|\n",
      "|1  |[at, bat, bat, cat, at]|(3,[0,1,2],[2.0,2.0,1.0])|\n",
      "+---+-----------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "raw_df = spark.createDataFrame([\n",
    "    (0, 'at bat cat'.split(' ')),\n",
    "    (1, 'at bat bat cat at'.split(' '))\n",
    "], ['id', 'words'])\n",
    "\n",
    "cv = CountVectorizer(inputCol='words', outputCol='features', vocabSize=3, minDF=2.0)\n",
    "model = cv.fit(raw_df)\n",
    "\n",
    "result = model.transform(raw_df)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature hasher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+---+--------------------------------------------------------+\n",
      "|x1 |x2   |x3 |x4 |features                                                |\n",
      "+---+-----+---+---+--------------------------------------------------------+\n",
      "|2.2|true |a  |cat|(262144,[35046,56751,184035,244783],[1.0,1.0,2.2,1.0])  |\n",
      "|4.5|false|b  |dog|(262144,[162446,179156,184035,223707],[1.0,1.0,4.5,1.0])|\n",
      "|4.4|false|c  |dog|(262144,[121161,162446,179156,184035],[1.0,1.0,1.0,4.4])|\n",
      "|2.3|true |d  |cat|(262144,[5506,35046,56751,184035],[1.0,1.0,1.0,2.3])    |\n",
      "+---+-----+---+---+--------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import FeatureHasher\n",
    "\n",
    "raw_df = spark.createDataFrame([\n",
    "    (2.2, True, 'a', 'cat'),\n",
    "    (4.5, False, 'b', 'dog'),\n",
    "    (4.4, False, 'c', 'dog'),\n",
    "    (2.3, True, 'd', 'cat')\n",
    "], ['x1', 'x2', 'x3', 'x4'])\n",
    "\n",
    "hasher = FeatureHasher(inputCols=['x1', 'x2', 'x3', 'x4'], outputCol='features')\n",
    "\n",
    "featurized = hasher.transform(raw_df)\n",
    "featurized.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------------------------------+------+\n",
      "|title                   |words                         |tokens|\n",
      "+------------------------+------------------------------+------+\n",
      "|How to program in Java  |[how, to, program, in, java]  |5     |\n",
      "|Java recipies           |[java, recipies]              |2     |\n",
      "|Learn Java in 24 hours  |[learn, java, in, 24, hours]  |5     |\n",
      "|How to program in Python|[how, to, program, in, python]|5     |\n",
      "|Python recipies         |[python, recipies]            |2     |\n",
      "|Learn Python in 24 hours|[learn, python, in, 24, hours]|5     |\n",
      "+------------------------+------------------------------+------+\n",
      "\n",
      "+------------------------+------------------------------+------+\n",
      "|title                   |words                         |tokens|\n",
      "+------------------------+------------------------------+------+\n",
      "|How to program in Java  |[how, to, program, in, java]  |5     |\n",
      "|Java recipies           |[java, recipies]              |2     |\n",
      "|Learn Java in 24 hours  |[learn, java, in, 24, hours]  |5     |\n",
      "|How to program in Python|[how, to, program, in, python]|5     |\n",
      "|Python recipies         |[python, recipies]            |2     |\n",
      "|Learn Python in 24 hours|[learn, python, in, 24, hours]|5     |\n",
      "+------------------------+------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "raw_df = spark.createDataFrame([\n",
    "    (1, 'How to program in Java'),\n",
    "    (2, 'Java recipies'),\n",
    "    (3, 'Learn Java in 24 hours'),\n",
    "    (4, 'How to program in Python'),\n",
    "    (5, 'Python recipies'),\n",
    "    (6, 'Learn Python in 24 hours')\n",
    "], ['id', 'title'])\n",
    "\n",
    "tokenizer1 = Tokenizer(inputCol='title', outputCol='words')\n",
    "tokenizer2 = RegexTokenizer(inputCol='title', outputCol='words', pattern=\"\\\\W\")\n",
    "\n",
    "tokenized1 = tokenizer1.transform(raw_df)\n",
    "tokenized2 = tokenizer2.transform(raw_df)\n",
    "\n",
    "counter = udf(lambda words: len(words), IntegerType())\n",
    "tokenized1\\\n",
    "    .select('title', 'words')\\\n",
    "    .withColumn('tokens', counter(col('words')))\\\n",
    "    .show(truncate=False)\n",
    "tokenized1\\\n",
    "    .select('title', 'words') \\\n",
    "    .withColumn('tokens', counter(col('words')))\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------+--------------------------+\n",
      "|id |title                         |filtered                  |\n",
      "+---+------------------------------+--------------------------+\n",
      "|1  |[How, to, program, in, Java]  |[program, Java]           |\n",
      "|2  |[Java, recipies]              |[Java, recipies]          |\n",
      "|3  |[Learn, Java, in, 24, hours]  |[Learn, Java, 24, hours]  |\n",
      "|4  |[How, to, program, in, Python]|[program, Python]         |\n",
      "|5  |[Python, recipies]            |[Python, recipies]        |\n",
      "|6  |[Learn, Python, in, 24, hours]|[Learn, Python, 24, hours]|\n",
      "+---+------------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "raw_df = spark.createDataFrame([\n",
    "    (1, 'How to program in Java'.split(' ')),\n",
    "    (2, 'Java recipies'.split(' ')),\n",
    "    (3, 'Learn Java in 24 hours'.split(' ')),\n",
    "    (4, 'How to program in Python'.split(' ')),\n",
    "    (5, 'Python recipies'.split(' ')),\n",
    "    (6, 'Learn Python in 24 hours'.split(' '))\n",
    "], ['id', 'title'])\n",
    "\n",
    "remover = StopWordsRemover(inputCol='title', outputCol='filtered')\n",
    "remover.transform(raw_df).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+\n",
      "|ngrams                                     |\n",
      "+-------------------------------------------+\n",
      "|[How to, to program, program in, in Java]  |\n",
      "|[Java recipies]                            |\n",
      "|[Learn Java, Java in, in 24, 24 hours]     |\n",
      "|[How to, to program, program in, in Python]|\n",
      "|[Python recipies]                          |\n",
      "|[Learn Python, Python in, in 24, 24 hours] |\n",
      "+-------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "raw_df = spark.createDataFrame([\n",
    "    (1, 'How to program in Java'.split(' ')),\n",
    "    (2, 'Java recipies'.split(' ')),\n",
    "    (3, 'Learn Java in 24 hours'.split(' ')),\n",
    "    (4, 'How to program in Python'.split(' ')),\n",
    "    (5, 'Python recipies'.split(' ')),\n",
    "    (6, 'Learn Python in 24 hours'.split(' '))\n",
    "], ['id', 'title'])\n",
    "\n",
    "ngram = NGram(n=2, inputCol='title', outputCol='ngrams')\n",
    "\n",
    "ngramDataFrame = ngram.transform(raw_df)\n",
    "ngramDataFrame.select('ngrams').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binarizer output with Threshold = 0.5\n",
      "+---+-------+-----------------+\n",
      "| id|feature|binarized_feature|\n",
      "+---+-------+-----------------+\n",
      "|  0|    0.1|              0.0|\n",
      "|  1|    0.8|              1.0|\n",
      "|  2|    0.2|              0.0|\n",
      "+---+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "\n",
    "raw_df = spark.createDataFrame([\n",
    "    (0, 0.1),\n",
    "    (1, 0.8),\n",
    "    (2, 0.2)\n",
    "], ['id', 'feature'])\n",
    "\n",
    "binarizer = Binarizer(threshold=0.5, inputCol='feature', outputCol='binarized_feature')\n",
    "bin_df = binarizer.transform(raw_df)\n",
    "\n",
    "print(f'Binarizer output with Threshold = {binarizer.getThreshold()}')\n",
    "bin_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal component analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------------------------------------------------+-----------------------------------------+\n",
      "|label|features                                                      |pca_features                             |\n",
      "+-----+--------------------------------------------------------------+-----------------------------------------+\n",
      "|1.0  |[-0.1398124360852646,1.9404337498133493,0.4441642252501767]   |[-1.7146272018673947,0.14595662137262444]|\n",
      "|0.0  |[-3.1210741978178023,-2.376881942973619,-0.009668122604746449]|[2.977397753291397,-2.234391686043509]   |\n",
      "|0.0  |[-1.4675982457620498,-1.1426554535428304,-0.3958504784847716] |[1.3403558685391892,-1.3153421310337556] |\n",
      "|1.0  |[-0.6816497596706191,1.2805160706325531,-0.1380411277034303]  |[-1.0867726484463247,-0.6289265701421938]|\n",
      "|0.0  |[0.6329591387069822,-0.5435350586379027,-0.2260626440162484]  |[0.3209687438850598,0.32628910847052517] |\n",
      "|1.0  |[-1.1306477069338503,2.506180828180563,-0.7930780368681248]   |[-2.2856106973303487,-1.4360947516967975]|\n",
      "|1.0  |[-1.0858904463756514,1.449819729192644,-2.0470674393378254]   |[-1.5606498256322983,-2.2249862380766023]|\n",
      "|0.0  |[-0.31513865899791305,-0.9013733970213405,1.2737757744041067] |[1.2000632834593374,0.6567660856334425]  |\n",
      "|0.0  |[1.0463616288162525,-1.0554542219563587,1.6279535580599434]   |[1.1061809462255545,1.9010543059395633]  |\n",
      "|1.0  |[0.7147386521308495,1.2019792718852083,1.75902821212546]      |[-0.9315862509771586,1.6852330903616992] |\n",
      "+-----+--------------------------------------------------------------+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "X, y = make_classification(**{\n",
    "    'n_samples': 10,\n",
    "    'n_features': 3,\n",
    "    'n_informative': 2,\n",
    "    'n_redundant': 0,\n",
    "    'n_repeated': 0,\n",
    "    'n_classes': 2,\n",
    "    'n_clusters_per_class': 2,\n",
    "    'random_state': 37\n",
    "})\n",
    "\n",
    "raw_df = spark.createDataFrame(\n",
    "    [(float(y[r]), Vectors.dense(X[r,:].tolist())) for r in range(X.shape[0])], \n",
    "    ['label', 'features'])\n",
    "\n",
    "pca = PCA(k=2, inputCol='features', outputCol='pca_features')\n",
    "model = pca.fit(raw_df)\n",
    "\n",
    "result = model.transform(raw_df)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------+\n",
      "| id|category|category_index|\n",
      "+---+--------+--------------+\n",
      "|  0|     rat|           0.0|\n",
      "|  1|     bat|           2.0|\n",
      "|  2|     cat|           1.0|\n",
      "|  3|     rat|           0.0|\n",
      "|  4|     rat|           0.0|\n",
      "|  5|     cat|           1.0|\n",
      "+---+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "raw_df = spark.createDataFrame([\n",
    "    (0, 'rat'), \n",
    "    (1, 'bat'), \n",
    "    (2, 'cat'), \n",
    "    (3, 'rat'), \n",
    "    (4, 'rat'), \n",
    "    (5, 'cat')], ['id', 'category'])\n",
    "\n",
    "StringIndexer(inputCol='category', outputCol='category_index')\\\n",
    "    .fit(raw_df)\\\n",
    "    .transform(raw_df)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------+-------------+\n",
      "|is_male|is_adult|  is_male_vec| is_adult_vec|\n",
      "+-------+--------+-------------+-------------+\n",
      "|    0.0|     1.0|(2,[0],[1.0])|(2,[1],[1.0])|\n",
      "|    0.0|     0.0|(2,[0],[1.0])|(2,[0],[1.0])|\n",
      "|    1.0|     1.0|(2,[1],[1.0])|(2,[1],[1.0])|\n",
      "|    1.0|     0.0|(2,[1],[1.0])|(2,[0],[1.0])|\n",
      "|    2.0|     2.0|    (2,[],[])|    (2,[],[])|\n",
      "+-------+--------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "\n",
    "raw_df = spark.createDataFrame([\n",
    "    (0.0, 1.0),\n",
    "    (0.0, 0.0),\n",
    "    (1.0, 1.0),\n",
    "    (1.0, 0.0),\n",
    "    (2.0, 2.0)\n",
    "], ['is_male', 'is_adult'])\n",
    "\n",
    "encoder = OneHotEncoderEstimator(inputCols=['is_male', 'is_adult'], outputCols=['is_male_vec', 'is_adult_vec'])\n",
    "model = encoder.fit(raw_df)\n",
    "\n",
    "encoded = model.transform(raw_df)\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----+------------------+\n",
      "| id|homework|exam|             grade|\n",
      "+---+--------+----+------------------+\n",
      "|  0|    0.98|0.88|              0.94|\n",
      "|  1|    0.88|0.77|0.8360000000000001|\n",
      "+---+--------+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import SQLTransformer\n",
    "\n",
    "raw_df = spark.createDataFrame([\n",
    "    (0, 0.98, 0.88),\n",
    "    (1, 0.88, 0.77)\n",
    "], ['id', 'homework', 'exam'])\n",
    "\n",
    "transformer = SQLTransformer(\n",
    "    statement='SELECT *, (0.6 * homework + 0.4 * exam) as grade FROM __THIS__')\n",
    "transformer.transform(raw_df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----------+----------+\n",
      "| x1| x2|x1_imputed|x2_imputed|\n",
      "+---+---+----------+----------+\n",
      "|1.0|NaN|       1.0|       4.0|\n",
      "|2.0|NaN|       2.0|       4.0|\n",
      "|NaN|3.0|       3.0|       3.0|\n",
      "|4.0|4.0|       4.0|       4.0|\n",
      "|5.0|5.0|       5.0|       5.0|\n",
      "+---+---+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "raw_df = spark.createDataFrame([\n",
    "    (1.0, float('nan')),\n",
    "    (2.0, float('nan')),\n",
    "    (float('nan'), 3.0),\n",
    "    (4.0, 4.0),\n",
    "    (5.0, 5.0)\n",
    "], ['x1', 'x2'])\n",
    "\n",
    "Imputer(inputCols=['x1', 'x2'], outputCols=['x1_imputed', 'x2_imputed'])\\\n",
    "    .fit(raw_df)\\\n",
    "    .transform(raw_df)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector slicer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+\n",
      "| user_features|features|\n",
      "+--------------+--------+\n",
      "|[-1.0,1.3,3.0]|   [1.3]|\n",
      "|[-2.0,2.3,0.0]|   [2.3]|\n",
      "+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorSlicer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.types import Row\n",
    "\n",
    "raw_df = spark.createDataFrame([\n",
    "    Row(user_features=Vectors.dense([-1.0, 1.3, 3.0])),\n",
    "    Row(user_features=Vectors.dense([-2.0, 2.3, 0.0]))])\n",
    "\n",
    "VectorSlicer(inputCol='user_features', outputCol='features', indices=[1])\\\n",
    "    .transform(raw_df)\\\n",
    "    .select('user_features', 'features')\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|              label|         prediction|\n",
      "+-------------------+-------------------+\n",
      "|-156.97053337870528|-159.70341330046185|\n",
      "| 28.805518697488676| 30.151952001502938|\n",
      "|  53.26548360120433|  50.65135193096635|\n",
      "|  76.47476360452336|   77.0946010330639|\n",
      "|-33.355650442108725|-36.325758608140816|\n",
      "|-3.6605327490473867|  -9.28390361398866|\n",
      "| -74.72068617488857|   -72.259527862171|\n",
      "| 22.540384098077414| 19.046555779007523|\n",
      "|  6.261118649116621| 12.180035472252564|\n",
      "| 31.669985186286436| 29.840281061212405|\n",
      "+-------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "X, y = make_regression(**{\n",
    "    'n_samples': 100,\n",
    "    'n_features': 4,\n",
    "    'n_informative': 4,\n",
    "    'n_targets': 1,\n",
    "    'bias': 5.3,\n",
    "    'random_state': 37\n",
    "})\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(float(y[r]), Vectors.dense(X[r,:].tolist())) for r in range(X.shape[0])], \n",
    "    ['label', 'features'])\n",
    "\n",
    "LinearRegression(labelCol='label', featuresCol='features', \n",
    "                 maxIter=10, regParam=0.3, elasticNetParam=0.8)\\\n",
    "    .fit(df).transform(df)\n",
    "\n",
    "GeneralizedLinearRegression(labelCol='label', featuresCol='features', \n",
    "                            family='gaussian', link='identity', \n",
    "                            maxIter=10, regParam=0.3)\\\n",
    "    .fit(df).transform(df)\n",
    "\n",
    "DecisionTreeRegressor(labelCol='label', featuresCol='features')\\\n",
    "    .fit(df).transform(df)\n",
    "\n",
    "RandomForestRegressor(labelCol='label', featuresCol='features')\\\n",
    "    .fit(df).transform(df)\n",
    "\n",
    "GBTRegressor(labelCol='label', featuresCol='features', maxIter=10)\\\n",
    "    .fit(df).transform(df).select('label', 'prediction').show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  0.0|       0.0|\n",
      "|  1.0|       1.0|\n",
      "+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "\n",
    "X, y = make_classification(**{\n",
    "    'n_samples': 100,\n",
    "    'n_features': 2,\n",
    "    'n_informative': 2,\n",
    "    'n_redundant': 0,\n",
    "    'n_repeated': 0,\n",
    "    'n_classes': 2,\n",
    "    'n_clusters_per_class': 2,\n",
    "    'random_state': 37\n",
    "})\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(float(y[r]), Vectors.dense(X[r,:].tolist())) for r in range(X.shape[0])], \n",
    "    ['label', 'features'])\n",
    "\n",
    "LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\\\n",
    "    .fit(df)\\\n",
    "    .transform(df)\n",
    "\n",
    "Pipeline(stages=[\n",
    "    StringIndexer(inputCol='label', outputCol='indexed_label'),\n",
    "    VectorIndexer(inputCol='features', outputCol='indexed_features', maxCategories=4),\n",
    "    DecisionTreeClassifier(labelCol='label', featuresCol='features')])\\\n",
    "    .fit(df).transform(df)\n",
    "\n",
    "Pipeline(stages=[\n",
    "    StringIndexer(inputCol='label', outputCol='indexed_label'),\n",
    "    VectorIndexer(inputCol='features', outputCol='indexed_features', maxCategories=4),\n",
    "    RandomForestClassifier(labelCol='label', featuresCol='features', numTrees=10)])\\\n",
    "    .fit(df).transform(df)\n",
    "\n",
    "Pipeline(stages=[\n",
    "    StringIndexer(inputCol='label', outputCol='indexed_label'),\n",
    "    VectorIndexer(inputCol='features', outputCol='indexed_features', maxCategories=4),\n",
    "    GBTClassifier(labelCol='label', featuresCol='features', maxIter=10)])\\\n",
    "    .fit(df).transform(df)\n",
    "\n",
    "MultilayerPerceptronClassifier(labelCol='label', featuresCol='features', \n",
    "                               maxIter=100, layers=[2, 5, 4, 2], \n",
    "                               blockSize=128, seed=37)\\\n",
    "    .fit(df).transform(df)\n",
    "\n",
    "LinearSVC(labelCol='label', featuresCol='features', maxIter=10, regParam=0.1)\\\n",
    "    .fit(df).transform(df).select('label', 'prediction').show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|  1.0|         0|\n",
      "|  0.0|         1|\n",
      "|  0.0|         1|\n",
      "|  1.0|         0|\n",
      "|  0.0|         1|\n",
      "|  1.0|         0|\n",
      "|  0.0|         1|\n",
      "|  1.0|         0|\n",
      "|  0.0|         1|\n",
      "|  0.0|         1|\n",
      "+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "X, y = make_blobs(**{\n",
    "    'n_samples': 3000,\n",
    "    'n_features': 15,\n",
    "    'centers': 2,\n",
    "    'cluster_std': 1.0,\n",
    "    'center_box': (-10.0, 10.0),\n",
    "    'random_state': 37\n",
    "})\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(float(y[r]), Vectors.dense(X[r,:].tolist())) for r in range(X.shape[0])], \n",
    "    ['label', 'features'])\n",
    "\n",
    "KMeans(featuresCol='features').setK(2).setSeed(37).fit(df).transform(df)\\\n",
    "    .select('label', 'prediction').show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent pattern mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+\n",
      "|    items|freq|\n",
      "+---------+----+\n",
      "|      [1]|   3|\n",
      "|      [2]|   3|\n",
      "|   [2, 1]|   3|\n",
      "|      [5]|   2|\n",
      "|   [5, 2]|   2|\n",
      "|[5, 2, 1]|   2|\n",
      "|   [5, 1]|   2|\n",
      "+---------+----+\n",
      "\n",
      "+----------+----------+------------------+----+\n",
      "|antecedent|consequent|        confidence|lift|\n",
      "+----------+----------+------------------+----+\n",
      "|    [5, 2]|       [1]|               1.0| 1.0|\n",
      "|       [2]|       [1]|               1.0| 1.0|\n",
      "|       [2]|       [5]|0.6666666666666666| 1.0|\n",
      "|    [2, 1]|       [5]|0.6666666666666666| 1.0|\n",
      "|       [5]|       [2]|               1.0| 1.0|\n",
      "|       [5]|       [1]|               1.0| 1.0|\n",
      "|    [5, 1]|       [2]|               1.0| 1.0|\n",
      "|       [1]|       [2]|               1.0| 1.0|\n",
      "|       [1]|       [5]|0.6666666666666666| 1.0|\n",
      "+----------+----------+------------------+----+\n",
      "\n",
      "+---+------------+----------+\n",
      "| id|       items|prediction|\n",
      "+---+------------+----------+\n",
      "|  0|   [1, 2, 5]|        []|\n",
      "|  1|[1, 2, 3, 5]|        []|\n",
      "|  2|      [1, 2]|       [5]|\n",
      "+---+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.fpm import FPGrowth\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0, [1, 2, 5]),\n",
    "    (1, [1, 2, 3, 5]),\n",
    "    (2, [1, 2])\n",
    "], ['id', 'items'])\n",
    "\n",
    "fpg = FPGrowth(itemsCol='items', minSupport=0.5, minConfidence=0.6)\n",
    "model = fpg.fit(df)\n",
    "\n",
    "model.freqItemsets.show()\n",
    "model.associationRules.show()\n",
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "raw_df = spark.createDataFrame([\n",
    "    (1, 'How to program in Java', 1.0),\n",
    "    (2, 'Java recipies', 1.0),\n",
    "    (3, 'Learn Java in 24 hours', 1.0),\n",
    "    (4, 'How to program in Python', 0.0),\n",
    "    (5, 'Python recipies', 0.0),\n",
    "    (6, 'Learn Python in 24 hours', 0.0)\n",
    "], ['id', 'title', 'label'])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='title', outputCol='words')\n",
    "hasher = HashingTF(inputCol='words', outputCol='features')\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hasher, lr])\n",
    "\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(hasher.numFeatures, [10, 100, 1000]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "cross_val = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=param_grid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=2)\n",
    "\n",
    "cv_model = cross_val.fit(raw_df)\n",
    "cv_model.transform(raw_df).select('label', 'prediction').show(n=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
