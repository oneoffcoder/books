{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resilient Distributed Datasets (RDD)\n",
    "\n",
    "Think of a RDD as a distributed dataset. From a Pythonic point of view, imagine a list of integers. This list of integers is one dataset and sits on one computer. \n",
    "\n",
    "```python\n",
    "data = [18, 19, 21, 17]\n",
    "```\n",
    "\n",
    "Now, imagine we could somehow split this dataset into two parts and place them on different computers.\n",
    "\n",
    "```python\n",
    "data_part_1 = [18, 19] # goes to computer 1\n",
    "data_part_2 = [21, 17] # goes to computer 2\n",
    "```\n",
    "\n",
    "At its most basic level, an RDD is conceptually a collection of elements that is spread around different computers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquiring a RDD\n",
    "\n",
    "How do we create a RDD or where does an RDD come from? RDDs may be created programmatically or from reading files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a RDD\n",
    "\n",
    "The easiest way to programmatically create an RDD is to use the `parallelize()` method from the **spark context** `sc`. Note that we pass in a list of numbers; the list of numbers is generated using a `list comprehension`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rdd = sc.parallelize([i for i in range(10)])\n",
    "type(num_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a pair RDD\n",
    "\n",
    "Just think of a `pair RDD` as a distributed dataset whose records are key-value pairs. From a Pythonic point of view, think about a list of tuples. This list of tuples is one dataset and sits on one computer. For each tuple in this list, \n",
    "* the first element is a name and plays the role of the `key`, and \n",
    "* the second element is an age and plays the role of the `value`. \n",
    "\n",
    "```python\n",
    "data = [('john', 18), ('jack', 19), ('jill', 21), ('jenn', 17)]\n",
    "```\n",
    "\n",
    "Now, imagine we could somehow split this dataset into two parts and place them on different computers.\n",
    "\n",
    "```python\n",
    "data_part_1 = [('john', 18), ('jack', 19)] # goes to computer 1\n",
    "data_part_2 = [('jill', 21), ('jenn', 17)] # goes to computer 2\n",
    "```\n",
    "\n",
    "At its most basic level, a pair RDD is conceptually a collection of 2-tuples that is spread around different computers. Below, we create a pair RDD where the key is a number and the value is the key multiplied by itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_rdd = sc.parallelize([(i, i*i) for i in range(10)])\n",
    "type(num_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read a RDD from HDFS\n",
    "\n",
    "If we store a `CSV` file in `HDFS` (Hadoop Distributed File System), we can read the contents into a RDD via `sc.textFile()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rdd = sc.textFile('hdfs://localhost/data.csv')\n",
    "type(data_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "\n",
    "After we acquire a RDD, we can do two broad categories of operations.\n",
    "\n",
    "* Transformation: an operation to change the data\n",
    "* Action: an operation to collect the data\n",
    "\n",
    "Transformation operations are `lazily` evaluated. Just because you have applied a transformation to a RDD does not mean anything will happen. Only when you execute an action against the RDD does computation actually start. Let's look at some types of transformations that we may perform against RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map\n",
    "\n",
    "The ``map()`` function transforms each element into something else. Below, we transform the original number into a new ones by\n",
    "\n",
    "* multiplying that number by itself,\n",
    "* adding one to that number,\n",
    "* subtracting one from that number, and\n",
    "* dividing that number by ten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rdd.map(lambda x: x * x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rdd.map(lambda x: x + 1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, 0, 1, 2, 3, 4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rdd.map(lambda x: x - 1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rdd.map(lambda x: x / 10).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter\n",
    "\n",
    "The ``filter()`` method removes elements from a RDD. The filter method must supply a function that returns `True` (to keep) or `False` (to remove) each element. Below, we filter even and odd elements out of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rdd.filter(lambda x: x % 2 == 0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rdd.filter(lambda x: x % 2 != 0).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flat map\n",
    "\n",
    "The `flatMap()` function flattens lists of lists into a list of elements. Let's say we have the following list of list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want to do is to flatten this list `data` so that the resulting list is as follows.\n",
    "\n",
    "```python\n",
    "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "```\n",
    "\n",
    "How do we flatten a list of lists in Python? In Python we can use the `chain()` method from the `itertools` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "list(chain(*data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using PySpark, the `flatMap()` function does the flattening for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "sc.parallelize(data).flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample\n",
    "\n",
    "If we want to take samples from a RDD, we can use the `sample()` method. The arguments to `sample()` are as follows.\n",
    "\n",
    "* `withReplacement` will indicate if we want to sample with replacement (records can be selected multiple times)\n",
    "* `fraction` specifies the percentage of the data we want to bring back\n",
    "* `seed` will be the seed used to control for randomization during sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3, 7]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rdd.sample(withReplacement=False, fraction=0.2, seed=37).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Union\n",
    "\n",
    "If we have two RDDs, we can bring them together through `union()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rdd.union(num_rdd).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersection\n",
    "\n",
    "Between two RDDs, if we want only the elements they share in common, we can apply the `intersection()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([1, 2, 3])\n",
    "b = sc.parallelize([3, 4, 5])\n",
    "\n",
    "a.intersection(b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distinct\n",
    "\n",
    "The `distinct()` function will bring back only unique elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([1, 2, 2, 3, 4])\n",
    "a.distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group by key\n",
    "\n",
    "If we have a pair RDD, we can group data by the key using `groupByKey()`. After we apply `groupByKey()` a new pair RDD is created where \n",
    "* the key is the key as before and \n",
    "* the value is an `iterable`.\n",
    "\n",
    "Below, we convert the `iterable` to a list using the `list()` function. The `groupByKey()` is an expensive operation as it causes data shuffling. In the Spark framework, we work extra hard to keep data from moving (as there is a lot of data and we do not desire to congest the network with such movement of huge data); the only thing we desire to move is the compute code. Try to avoid `groupByKey()` when you can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [1, 2]\n",
      "2 [4, 3]\n"
     ]
    }
   ],
   "source": [
    "a = sc.parallelize([(1, 1), (1, 2), (2, 4), (2, 3)])\n",
    "\n",
    "for key, it in a.groupByKey().collect():\n",
    "    print(key, list(it))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce by key\n",
    "\n",
    "If we wanted to collapse all the values associated with a key in a pair RDD, we need to use the `reduceByKey()` function. The `reduceByKey()` function is much more efficient than `groupByKey()`. We should work extra hard to modify logic that works for `groupByKey()` to work for and use `reduceByKey()`.\n",
    "\n",
    "Below, we simply sum over all the values associated with a key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 3), (2, 7)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([(1, 1), (1, 2), (2, 4), (2, 3)])\n",
    "a.reduceByKey(lambda a, b: a + b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a sort of an `anti-pattern` using `groupByKey()` to add the elements associated with each key. We get the same result as with `reduceByKey()`, but with potentially extra overhead (data shuffling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 3), (2, 7)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_elements = lambda tup: (tup[0], sum(list(tup[1])))\n",
    "\n",
    "sc.parallelize([(1, 1), (1, 2), (2, 4), (2, 3)])\\\n",
    "    .groupByKey()\\\n",
    "    .map(add_elements)\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an interesting dataset. It's a list of 2-tuples, where the first element is the key or unique identifier of a person, and the second element is a piece of information (stored in a map) about the person. How do we use `reduceByKey()` to merge all the information according to the unique identifier? If you look below, you will notice that we merge the dictionaries using the **dictionary unpacking operator** `**`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, {'name': 'john', 'age': 23}), (2, {'name': 'jack', 'age': 24})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    (1, {'name': 'john'}),\n",
    "    (2, {'name': 'jack'}),\n",
    "    (1, {'age': 23}),\n",
    "    (2, {'age': 24}),\n",
    "]\n",
    "\n",
    "sc.parallelize(data).reduceByKey(lambda a, b: {**a, **b}).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate by key\n",
    "\n",
    "In a pair RDD, we can specify how to aggregate values by keys **within** and **between** partitions. There are three arguments required.\n",
    "\n",
    "* an initial value\n",
    "* a combining function to aggregate within a partition\n",
    "* a merging function to aggregate between partitions\n",
    "\n",
    "Below, are some examples of how to aggregate by key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'value 1, value 2'), (2, 'value 4, value 3')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([(1, 1), (1, 2), (2, 4), (2, 3)])\n",
    "a.aggregateByKey('value', lambda s, d: f'{s} {d}', lambda s1, s2: f'{s1}, {s2}').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 3), (2, 7)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([(1, 1), (1, 2), (2, 4), (2, 3)])\n",
    "a.aggregateByKey(0, lambda s, d: s + d, lambda s1, s2: s1 + s2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort by key\n",
    "\n",
    "We can also sort records by key in a pair RDD using `sortByKey()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (3, 2), (4, 3), (5, 4)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([(1, 1), (3, 2), (5, 4), (4, 3)])\n",
    "a.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join\n",
    "\n",
    "If we have two pair RDDs, we can perform a join based on the keys using `join()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (1, 2)), (2, (2, 3)), (3, (3, 4))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([(1, 1), (2, 2), (3, 3)])\n",
    "b = sc.parallelize([(1, 2), (2, 3), (3, 4)])\n",
    "a.join(b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `join()` is like a SQL inner join; only records with keys in both RDDs will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (1, 2)), (2, (2, 3)), (3, (3, 4))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([(1, 1), (2, 2), (3, 3), (5, 5)])\n",
    "b = sc.parallelize([(1, 2), (2, 3), (3, 4), (6, 6)])\n",
    "a.join(b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left outer join\n",
    "\n",
    "The `leftOuterJoin()` will join two pair RDDs like a SQL left-outer join. All records on the left will be returned even if there is not a corresponding matching record on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (1, 2)), (2, (2, 3)), (3, (3, None))]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([(1, 1), (2, 2), (3, 3)])\n",
    "b = sc.parallelize([(1, 2), (2, 3), (4, 5)])\n",
    "a.leftOuterJoin(b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Right outer join\n",
    "\n",
    "The `rightOuterJoin()` will join two pair RDDs like a SQL right-outer join. All records on the right will be returned even if there is not a corresponding matching record on the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (1, 2)), (2, (2, 3)), (4, (None, 5))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([(1, 1), (2, 2), (3, 3)])\n",
    "b = sc.parallelize([(1, 2), (2, 3), (4, 5)])\n",
    "a.rightOuterJoin(b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full outer join\n",
    "\n",
    "The `fullOuterJoin()` will join two pair RDDs like a SQL full-outer join. All records on the left and right will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (1, 2)), (2, (2, 3)), (3, (3, None)), (4, (None, 5))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([(1, 1), (2, 2), (3, 3)])\n",
    "b = sc.parallelize([(1, 2), (2, 3), (4, 5)])\n",
    "a.fullOuterJoin(b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cogroup\n",
    "\n",
    "The `cogroup()` function will bring the values from two pair RDDs together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [1, 2] ['a', 'b']\n",
      "2 [3, 4] ['c', 'd']\n",
      "3 [5, 6] ['e', 'f']\n"
     ]
    }
   ],
   "source": [
    "a = sc.parallelize([(1, 1), (1, 2), (2, 3), (2, 4), (3, 5), (3, 6)])\n",
    "b = sc.parallelize([(1, 'a'), (1, 'b'), (2, 'c'), (2, 'd'), (3, 'e'), (3, 'f')])\n",
    "\n",
    "for key, (it1, it2) in a.cogroup(b).collect():\n",
    "    print(key, list(it1), list(it2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cartesian\n",
    "\n",
    "In Python, if we had two list as follows, and we wanted the cartesian product of those two lists, we use `product` from the `itertools` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'a'),\n",
       " (1, 'b'),\n",
       " (1, 'c'),\n",
       " (1, 'd'),\n",
       " (2, 'a'),\n",
       " (2, 'b'),\n",
       " (2, 'c'),\n",
       " (2, 'd'),\n",
       " (3, 'a'),\n",
       " (3, 'b'),\n",
       " (3, 'c'),\n",
       " (3, 'd'),\n",
       " (4, 'a'),\n",
       " (4, 'b'),\n",
       " (4, 'c'),\n",
       " (4, 'd')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "a = [1, 2, 3, 4]\n",
    "b = ['a', 'b', 'c', 'd']\n",
    "\n",
    "list(product(*[a, b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can achieve the same using the `cartesian()` function on two RDDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'a'),\n",
       " (1, 'b'),\n",
       " (1, 'c'),\n",
       " (1, 'd'),\n",
       " (2, 'a'),\n",
       " (2, 'b'),\n",
       " (2, 'c'),\n",
       " (2, 'd'),\n",
       " (3, 'a'),\n",
       " (3, 'b'),\n",
       " (3, 'c'),\n",
       " (3, 'd'),\n",
       " (4, 'a'),\n",
       " (4, 'b'),\n",
       " (4, 'c'),\n",
       " (4, 'd')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([1, 2, 3, 4])\n",
    "b = sc.parallelize(['a', 'b', 'c', 'd'])\n",
    "a.cartesian(b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repartition\n",
    "\n",
    "We can force our distributed dataset (the RDD) into a specified number of partitions using `repartition()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "a = sc.parallelize(['hello', 'world'])\n",
    "print(a.getNumPartitions())\n",
    "\n",
    "a = a.repartition(2)\n",
    "print(a.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coalesce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also force our RDD into a specified number of partitions using `coalesce()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "a = sc.parallelize(['hello', 'world'])\n",
    "print(a.getNumPartitions())\n",
    "\n",
    "a = a.coalesce(2)\n",
    "print(a.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, `repartition()` and `coalesce()` seem to do the same thing: forcing the data into a specified number of partitions. What's the difference? \n",
    "\n",
    "* `repartition()` incurs a cost of a shuffling and creating new partitions, however, the resulting partitions are roughly equal in size.\n",
    "* `coalesce()` minimizes shuffling of data and reuses existing partitions, however, the result partitions will most likely not be roughly equal in size.\n",
    "\n",
    "Which should I use? It depends on your goals and/or preferences. If you want computation to be evenly distributed, go for `repartition()`, otherwise, save time by **not** shuffling data and use `coalesce()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipe\n",
    "\n",
    "The `pipe()` function enables you to specify an external script or program to transform the data. The script or program must be able to receive the data as input and should return an output. The script must be accessible on all the compute nodes; a common mistake is that the script only exists on the driver node and your piping fails. \n",
    "\n",
    "In the code below, by default, the RDD has 12 partitions, that is why we see 12 outputs of `One-Off Coder`. Obviously, or not, 10 of the partitions have no data and only 2 of them do (one for **hello** and one for **world**). The script is just a simple echo and looks like the following.\n",
    "\n",
    "```bash\n",
    "#!/bin/sh\n",
    "echo 'One-Off Coder'\n",
    "while read LINE; do\n",
    "    echo ${LINE}\n",
    "done\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['One-Off Coder',\n",
       " 'One-Off Coder',\n",
       " 'One-Off Coder',\n",
       " 'One-Off Coder',\n",
       " 'hello',\n",
       " 'One-Off Coder',\n",
       " 'One-Off Coder',\n",
       " 'One-Off Coder',\n",
       " 'One-Off Coder',\n",
       " 'world']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize(['hello', 'world'])\n",
    "a.pipe('/root/ipynb/echo.sh').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we force the number of partitions to 2, then we get more sensible output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['One-Off Coder', 'hello', 'One-Off Coder', 'world']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize(['hello', 'world']).repartition(2)\n",
    "a.pipe('/root/ipynb/echo.sh').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we force the number of partitions to 1, then all the data will be fed to one instance of the script (that's why we see `One-Off Coder` once only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['One-Off Coder', 'hello', 'world']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize(['hello', 'world']).repartition(1)\n",
    "a.pipe('/root/ipynb/echo.sh').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repartition and sort within partitions\n",
    "\n",
    "For a pair RDD, we can control how many partitions we want and which records go into which partitions with `repartitionAndSortWithinPartitions()`. What we get for free is sorting within each partition. The arguments for `repartitionAndSortWithinPartitions()` are as follows.\n",
    "\n",
    "* `numPartitions` specifies the number of desired partitions\n",
    "* `partitionFunc` specifies how to assign records to partitions\n",
    "* `ascending` specifies if we want to sort ascendingly\n",
    "* `keyfunc` specifies how to retrieve the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 13), (2, 14), (2, 15), (1, 3), (1, 4), (1, 5)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([(1, 5), (2, 15), (1, 4), (2, 14), (1, 3), (2, 13)])\\\n",
    "    .map(lambda tup: (tup, tup[1]))\\\n",
    "    .repartitionAndSortWithinPartitions(\n",
    "        numPartitions=2, \n",
    "        partitionFunc=lambda tup: tup[0] % 2)\\\n",
    "    .map(lambda tup: tup[0])\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions\n",
    "\n",
    "Remember, **transformations** on RDDs create other RDDs and are lazily evaluated (no computational cost is incurred). On the other hand, when an **action** is applied to a RDD, a non-RDD is the result and the data is typically returned to the driver node (or the user from the worker nodes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce\n",
    "\n",
    "The `reduce()` function collapses all the elements into one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([1, 2, 3])\n",
    "a.reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing data does not have to be math operations like adding. Below, we merge the dictionaries into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fname': 'john', 'lname': 'doe', 'age': 32}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([{'fname': 'john'}, {'lname': 'doe'}, {'age': 32}])\n",
    "a.reduce(lambda a, b: {**a, **b})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also reduce data by selecting on the smallest value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "a = sc.parallelize([randint(10, 1000) for _ in range(100)])\n",
    "a.reduce(lambda a, b: min(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect\n",
    "\n",
    "The `collect()` function is an action that we have been using all along. This function simply brings back the distributed data into one list on the driver. Be careful, though, as if the data is huge, this operation may fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([1, 2, 3])\n",
    "a.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count\n",
    "\n",
    "The `count()` function counts the number of elements in a RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([1, 2, 3])\n",
    "a.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we generate 1,000 random numbers in the range $[1, 10]$. We then perform a `map()` operation creating a list of $x$ length for each $x$, followed by a `flatMap()` and then `count()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5639"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "a = sc.parallelize([randint(1, 10) for _ in range(1000)])\n",
    "a.map(lambda x: [x for _ in range(x)]).flatMap(lambda x: x).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First\n",
    "\n",
    "The function `first()` always returns the first record back from a RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([1, 2, 3])\n",
    "a.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take\n",
    "\n",
    "We can bring back the first $n$ records using `take()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([1, 2, 3])\n",
    "a.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take sample\n",
    "\n",
    "We can bring back random records using `takeSample()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21, 86, 84, 83, 26, 59, 92, 0, 48, 44]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([i for i in range(100)])\n",
    "a.takeSample(withReplacement=False, num=10, seed=37)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take ordered\n",
    "\n",
    "We can bring back the first $n$ records in order using `takeOrdered()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 40, 40, 55, 63, 68, 69, 81, 83, 106]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "a = sc.parallelize([randint(1, 10000) for _ in range(1000)])\n",
    "a.takeOrdered(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count by key\n",
    "\n",
    "Counting the number of records associated with a key is accomplished through `countByKey()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {2: 1045,\n",
       "             1: 972,\n",
       "             7: 1005,\n",
       "             8: 975,\n",
       "             10: 1044,\n",
       "             5: 1017,\n",
       "             4: 1008,\n",
       "             6: 904,\n",
       "             9: 999,\n",
       "             3: 1031})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([(randint(1, 10), 1) for _ in range(10000)])\n",
    "a.countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining transformations and actions\n",
    "\n",
    "The power of transformations and actions emerges from chaining them together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map, filter, reduce\n",
    "\n",
    "The three basic functions introduced when we start to adopt `functional programming` are `map()`, `filter()` and `reduce()`. Below, we map each number $x$ to $x \\times x$, filter for only even numbers, and then add the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rdd = sc.parallelize([i for i in range(10)])\n",
    "\n",
    "num_rdd\\\n",
    "    .map(lambda x: x * x)\\\n",
    "    .filter(lambda x: x % 2 == 0)\\\n",
    "    .reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter, map, take\n",
    "\n",
    "Here's an example of parsing out a `CSV` file. Note that we have to filter out the row starting with `x` since that indicates the header (for this CSV file). We then split (or tokenize) the line specifying the delimiter as a comma `,`. We finally convert all the tokens, which are strings, to integers. We take the first 10 records (rows) to see if we parsed the CSV file correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[14, 22, 25, 63, 47, 52, 13, 14, 23, 27],\n",
       " [35, 80, 38, 28, 73, 69, 21, 16, 76, 53],\n",
       " [46, 37, 46, 55, 78, 68, 61, 62, 81, 82],\n",
       " [19, 12, 45, 50, 71, 63, 94, 7, 10, 77],\n",
       " [50, 94, 94, 87, 67, 89, 73, 17, 39, 7],\n",
       " [47, 97, 64, 7, 47, 40, 77, 63, 50, 21],\n",
       " [33, 0, 99, 46, 43, 32, 47, 20, 4, 67],\n",
       " [46, 100, 28, 8, 34, 49, 62, 77, 4, 51],\n",
       " [14, 12, 50, 96, 57, 59, 40, 87, 44, 48],\n",
       " [13, 48, 30, 62, 88, 99, 65, 94, 13, 34]]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rdd = sc.textFile('hdfs://localhost/data.csv')\n",
    "\n",
    "data_rdd\\\n",
    "    .filter(lambda s: False if s.startswith('x') else True)\\\n",
    "    .map(lambda s: s.split(','))\\\n",
    "    .map(lambda arr: [int(s) for s in arr])\\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging dictionaries\n",
    "\n",
    "We already saw some examples of merging dictionaries. Here's another example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{8: 1001,\n",
       " 1: 985,\n",
       " 9: 1006,\n",
       " 10: 1002,\n",
       " 2: 969,\n",
       " 3: 980,\n",
       " 4: 984,\n",
       " 5: 1009,\n",
       " 6: 1010,\n",
       " 7: 1054}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([(randint(1, 10), 1) for _ in range(10000)])\\\n",
    "    .reduceByKey(lambda a, b: a + b)\\\n",
    "    .map(lambda tup: {tup[0]: tup[1]})\\\n",
    "    .reduce(lambda a, b: {**a, **b})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting variables\n",
    "\n",
    "If we have data that needs to be shared across the worker nodes, we can `broadcast` that data. Below, we have a dictionary `m` that is local to the driver and we want to broadcast (make it available) it to all the worker nodes. We broadcast `m` with `sc.broadcast()` and assign the reference to `b`; note that `b` wraps the data `m` and we can access the dictionary through `value` property of `b` (e.g. `b.value`). Now our parallel operations can access the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 2197),\n",
       " (1, 620),\n",
       " (9, 1600),\n",
       " (2, 1734),\n",
       " (10, 1630),\n",
       " (3, 2427),\n",
       " (4, 1920),\n",
       " (5, 1996),\n",
       " (6, 2664),\n",
       " (7, 3212)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "m = {i: randint(1, 10) for i in range(101)}\n",
    "b = sc.broadcast(m)\n",
    "\n",
    "sc.parallelize([randint(1, 100) for _ in range(20000)])\\\n",
    "    .map(lambda num: (b.value[num], 1))\\\n",
    "    .reduceByKey(lambda a, b: a + b)\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accumulator\n",
    "\n",
    "If we want to keep count of things or put metrics on our operations, we need to use an `accumulator`. The `accumulator` is defined locally (on the driver) but is visible across the worker nodes. Below, we use an accumulator to simply keep track of the number of map operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum = sc.accumulator(0)\n",
    "\n",
    "sc.parallelize([i for i in range(10000)])\\\n",
    "    .map(lambda num: accum.add(1))\\\n",
    "    .count()\n",
    "\n",
    "accum.value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
