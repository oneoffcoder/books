{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrames\n",
    "\n",
    "A simple way to think of a `DataFrame` in Spark is to see it as a distributed table of data or a distributed CSV file. However, this distributed table of data or CSV file comes highly glorified with bells and whistles and battery included. The highly prized features of Spark DataFrames is that they are not unlike `R` or `pandas` data frames and you can issue SQL-like commands against them. \n",
    "\n",
    "As a side note, there are three main distributed data structures in Spark. \n",
    "\n",
    "* `RDD`\n",
    "* `DataFrame`\n",
    "* `DataSet`\n",
    "\n",
    "The RDD data structure is the original distributed data structure and the records can be anything. RDDs were very friendly to experienced data engineers and programmers. DataFrames are a movement away RDDs, and provide tabular structure to records and made Spark accessible to other types of data programmers (such as those who are comfortable with SQL). Still, DataFrames were too generic, and DataSets were created to have the tabular structure of DataFrames where the records could be specifically defined. A DataFrame is just a DataSet, where the records are of the type `Record`, and a DataSet is said to be a strongly-typed, user-defined distributed, tabular data structure.\n",
    "\n",
    "It is said, when you are using a RDD, you are describing `how` you are doing something and when you are using a DataFrame or DataSet, you are describing `what` you are doing. The `how` you are doing something relates to the `imperative` programming paradigm, and the `what` you are doing relates to the `functional` programming paradigm. It is argued that code that tells `what` you are doing is easier to understand than code that tells `how` you are doing it. However, which one of these approaches is easier to understand differs from person to person. Sometimes, functional programming style of coding results in highly nested code. Take the example below assuming that we are composing behavior through the composition of functions.\n",
    "\n",
    "```python\n",
    "is(this(how(we(want(to(code))))))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquiring a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Pandas DataFrame to Spark DataFrame\n",
    "\n",
    "The easiest way to get a Spark DataFrame is to convert a pandas DataFrame to a Spark one. There's a convenience method from the `sqlContext` to do so, `createDataFrame()`. Below, the pandas DataFrame is set to `pdf` and the Spark DataFrame is set to `sdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from random import randint\n",
    "\n",
    "n_cols = 10\n",
    "n_rows = 10\n",
    "\n",
    "pdf = pd.DataFrame([tuple([c for c in range(n_cols)]) for r in range(n_rows)], columns=[f'x{i}' for i in range(n_cols)])\n",
    "sdf = sqlContext.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x0  x1  x2  x3  x4  x5  x6  x7  x8  x9\n",
       "0   0   1   2   3   4   5   6   7   8   9\n",
       "1   0   1   2   3   4   5   6   7   8   9\n",
       "2   0   1   2   3   4   5   6   7   8   9\n",
       "3   0   1   2   3   4   5   6   7   8   9\n",
       "4   0   1   2   3   4   5   6   7   8   9\n",
       "5   0   1   2   3   4   5   6   7   8   9\n",
       "6   0   1   2   3   4   5   6   7   8   9\n",
       "7   0   1   2   3   4   5   6   7   8   9\n",
       "8   0   1   2   3   4   5   6   7   8   9\n",
       "9   0   1   2   3   4   5   6   7   8   9"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the Spark DataFrame's records are of type `Row`? The `Row` behaves just like Python's `tuple` and `dictionary` types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(x0=0, x1=1, x2=2, x3=3, x4=4, x5=5, x6=6, x7=7, x8=8, x9=9),\n",
       " Row(x0=0, x1=1, x2=2, x3=3, x4=4, x5=5, x6=6, x7=7, x8=8, x9=9),\n",
       " Row(x0=0, x1=1, x2=2, x3=3, x4=4, x5=5, x6=6, x7=7, x8=8, x9=9),\n",
       " Row(x0=0, x1=1, x2=2, x3=3, x4=4, x5=5, x6=6, x7=7, x8=8, x9=9),\n",
       " Row(x0=0, x1=1, x2=2, x3=3, x4=4, x5=5, x6=6, x7=7, x8=8, x9=9),\n",
       " Row(x0=0, x1=1, x2=2, x3=3, x4=4, x5=5, x6=6, x7=7, x8=8, x9=9),\n",
       " Row(x0=0, x1=1, x2=2, x3=3, x4=4, x5=5, x6=6, x7=7, x8=8, x9=9),\n",
       " Row(x0=0, x1=1, x2=2, x3=3, x4=4, x5=5, x6=6, x7=7, x8=8, x9=9),\n",
       " Row(x0=0, x1=1, x2=2, x3=3, x4=4, x5=5, x6=6, x7=7, x8=8, x9=9),\n",
       " Row(x0=0, x1=1, x2=2, x3=3, x4=4, x5=5, x6=6, x7=7, x8=8, x9=9)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab the first row of this DataFrame and see how we can interact with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(x0=0, x1=1, x2=2, x3=3, x4=4, x5=5, x6=6, x7=7, x8=8, x9=9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = sdf.take(1)[0]\n",
    "row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the fields of a `Row` via index notation (like a tuple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(row)):\n",
    "    print(row[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also reference the values by keys in a row (like a dictionary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(row)):\n",
    "    key = f'x{i}'\n",
    "    print(row[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, it's just better to convert the row to a dictionary and then use our Python knowledge of iterating and manipulating dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0: 0\n",
      "x1: 1\n",
      "x2: 2\n",
      "x3: 3\n",
      "x4: 4\n",
      "x5: 5\n",
      "x6: 6\n",
      "x7: 7\n",
      "x8: 8\n",
      "x9: 9\n"
     ]
    }
   ],
   "source": [
    "for k, v in row.asDict().items():\n",
    "    print(f'{k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enough of `Rows`, let's get back to the Spark DataFrame. We can display the contents of a Spark DataFrame with `show()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+---+---+---+---+\n",
      "| x0| x1| x2| x3| x4| x5| x6| x7| x8| x9|\n",
      "+---+---+---+---+---+---+---+---+---+---+\n",
      "|  0|  1|  2|  3|  4|  5|  6|  7|  8|  9|\n",
      "|  0|  1|  2|  3|  4|  5|  6|  7|  8|  9|\n",
      "|  0|  1|  2|  3|  4|  5|  6|  7|  8|  9|\n",
      "|  0|  1|  2|  3|  4|  5|  6|  7|  8|  9|\n",
      "|  0|  1|  2|  3|  4|  5|  6|  7|  8|  9|\n",
      "|  0|  1|  2|  3|  4|  5|  6|  7|  8|  9|\n",
      "|  0|  1|  2|  3|  4|  5|  6|  7|  8|  9|\n",
      "|  0|  1|  2|  3|  4|  5|  6|  7|  8|  9|\n",
      "|  0|  1|  2|  3|  4|  5|  6|  7|  8|  9|\n",
      "|  0|  1|  2|  3|  4|  5|  6|  7|  8|  9|\n",
      "+---+---+---+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to inspect the schema of a DataFrame? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x0: long (nullable = true)\n",
      " |-- x1: long (nullable = true)\n",
      " |-- x2: long (nullable = true)\n",
      " |-- x3: long (nullable = true)\n",
      " |-- x4: long (nullable = true)\n",
      " |-- x5: long (nullable = true)\n",
      " |-- x6: long (nullable = true)\n",
      " |-- x7: long (nullable = true)\n",
      " |-- x8: long (nullable = true)\n",
      " |-- x9: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert a RDD to DataFrame\n",
    "\n",
    "A RDD can be converted to a DataFrame, however, we need to create a schema. Below is a concise way of creating a schema for a RDD. Notice that `createDataFrame()` is overloaded? Before, we passed in a pandas DataFrame. Here, we pass in a RDD and a schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "n_cols = 10\n",
    "n_rows = 10\n",
    "\n",
    "rdd = sc.parallelize([[c for c in range(n_cols)] for r in range(n_rows)])\n",
    "\n",
    "schema = StructType([StructField(f'x{i}', IntegerType(), True) for i in range(n_cols)])\n",
    "df = sqlContext.createDataFrame(rdd, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above example is too concise, let's do build the schema manually. The schema is defined by a `StructType` and the `StructType` is based off of a list of `StructFields`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_fields = [\n",
    "    StructField('x0', IntegerType(), True),\n",
    "    StructField('x1', IntegerType(), True),\n",
    "    StructField('x2', IntegerType(), True),\n",
    "    StructField('x3', IntegerType(), True),\n",
    "    StructField('x4', IntegerType(), True),\n",
    "    StructField('x5', IntegerType(), True),\n",
    "    StructField('x6', IntegerType(), True),\n",
    "    StructField('x7', IntegerType(), True),\n",
    "    StructField('x8', IntegerType(), True),\n",
    "    StructField('x9', IntegerType(), True)\n",
    "]\n",
    "\n",
    "struct_type = StructType(struct_fields)\n",
    "df = sqlContext.createDataFrame(rdd, struct_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the RDD. It's a list of lists (of integers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the DataFrame. It's a list of `Rows`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(x0=0, x1=1, x2=2, x3=3, x4=4, x5=5, x6=6, x7=7, x8=8, x9=9),\n",
       " Row(x0=0, x1=1, x2=2, x3=3, x4=4, x5=5, x6=6, x7=7, x8=8, x9=9),\n",
       " Row(x0=0, x1=1, x2=2, x3=3, x4=4, x5=5, x6=6, x7=7, x8=8, x9=9),\n",
       " Row(x0=0, x1=1, x2=2, x3=3, x4=4, x5=5, x6=6, x7=7, x8=8, x9=9),\n",
       " Row(x0=0, x1=1, x2=2, x3=3, x4=4, x5=5, x6=6, x7=7, x8=8, x9=9),\n",
       " Row(x0=0, x1=1, x2=2, x3=3, x4=4, x5=5, x6=6, x7=7, x8=8, x9=9),\n",
       " Row(x0=0, x1=1, x2=2, x3=3, x4=4, x5=5, x6=6, x7=7, x8=8, x9=9),\n",
       " Row(x0=0, x1=1, x2=2, x3=3, x4=4, x5=5, x6=6, x7=7, x8=8, x9=9),\n",
       " Row(x0=0, x1=1, x2=2, x3=3, x4=4, x5=5, x6=6, x7=7, x8=8, x9=9),\n",
       " Row(x0=0, x1=1, x2=2, x3=3, x4=4, x5=5, x6=6, x7=7, x8=8, x9=9)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a display of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+---+---+---+---+\n",
      "| x0| x1| x2| x3| x4| x5| x6| x7| x8| x9|\n",
      "+---+---+---+---+---+---+---+---+---+---+\n",
      "|  0|  1|  2|  3|  4|  5|  6|  7|  8|  9|\n",
      "|  0|  1|  2|  3|  4|  5|  6|  7|  8|  9|\n",
      "|  0|  1|  2|  3|  4|  5|  6|  7|  8|  9|\n",
      "|  0|  1|  2|  3|  4|  5|  6|  7|  8|  9|\n",
      "|  0|  1|  2|  3|  4|  5|  6|  7|  8|  9|\n",
      "|  0|  1|  2|  3|  4|  5|  6|  7|  8|  9|\n",
      "|  0|  1|  2|  3|  4|  5|  6|  7|  8|  9|\n",
      "|  0|  1|  2|  3|  4|  5|  6|  7|  8|  9|\n",
      "|  0|  1|  2|  3|  4|  5|  6|  7|  8|  9|\n",
      "|  0|  1|  2|  3|  4|  5|  6|  7|  8|  9|\n",
      "+---+---+---+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's inspect the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x0: integer (nullable = true)\n",
      " |-- x1: integer (nullable = true)\n",
      " |-- x2: integer (nullable = true)\n",
      " |-- x3: integer (nullable = true)\n",
      " |-- x4: integer (nullable = true)\n",
      " |-- x5: integer (nullable = true)\n",
      " |-- x6: integer (nullable = true)\n",
      " |-- x7: integer (nullable = true)\n",
      " |-- x8: integer (nullable = true)\n",
      " |-- x9: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert JSON data to Spark DataFrame\n",
    "\n",
    "We have seen how to create a Spark DataFrame from a pandas DataFrame or a RDD. Let's see how we can create a Spark DataFrame from reading a `JSON` file. First, let's upload the JSON file to `HDFS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-07 04:50:32,012 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "hdfs dfs -copyFromLocal -f /root/ipynb/people.json /people.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `sqlContext.read.json()` method to read the JSON file from HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.json('hdfs://localhost/people.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what's inside the Spark DataFrame. Notice how the Spark DataFrame is still tabular or table-ish? We know that JSON is a highly nested structure, and so where there's nesting, only the top-level keys are mapped to the fields/columns of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----------+------+---+---------+-----+--------------------+------+\n",
      "|             address|age|first_name|height| id|last_name| male|              sports|weight|\n",
      "+--------------------+---+----------+------+---+---------+-----+--------------------+------+\n",
      "|[Washington, DC, ...| 27|      John|   6.5|  1|      Doe| true|    [hockey, tennis]| 155.5|\n",
      "|[Washington, DC, ...| 22|      Jane|   5.7|  2|    Smith|false|[basketball, tennis]| 135.5|\n",
      "|[Los Angeles, CA,...| 25|      Jack|   6.6|  3|    Smith| true|  [baseball, soccer]| 175.5|\n",
      "|[Los Angeles, CA,...| 18|     Janet|   5.5|  4|      Doe|false|    [judo, baseball]| 125.5|\n",
      "+--------------------+---+----------+------+---+---------+-----+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But then, inspect the schema. The schema reflects the nested JSON data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- address: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- state: string (nullable = true)\n",
      " |    |-- street: string (nullable = true)\n",
      " |    |-- zip: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- male: boolean (nullable = true)\n",
      " |-- sports: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- weight: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame operations\n",
    "\n",
    "What can we actually do with a Spark DataFrame? Can we do amazing things with DataFrames as with RDDs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data\n",
    "\n",
    "Let's create a dummy Spark DataFrame first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from random import randint, choice\n",
    "\n",
    "def generate_num(col):\n",
    "    if col == 3:\n",
    "        p = randint(1, 100)\n",
    "        if p < 70:\n",
    "            return None\n",
    "        return randint(1, 10)\n",
    "    return randint(1, 10)\n",
    "\n",
    "def generate_height():\n",
    "    return choice(['tall', 'short'])\n",
    "\n",
    "n_cols = 10\n",
    "n_rows = 10\n",
    "\n",
    "pdf = pd.DataFrame(\n",
    "    [tuple([generate_height()] + [generate_num(c) for c in range(n_cols)]) for r in range(n_rows)], \n",
    "    columns=['height'] + [f'x{i}' for i in range(n_cols)])\n",
    "sdf = sqlContext.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>height</th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>short</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tall</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tall</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>short</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tall</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tall</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tall</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>short</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>short</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tall</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  height  x0  x1  x2   x3  x4  x5  x6  x7  x8  x9\n",
       "0  short   1   2   4  NaN   5   3   2   3   8   2\n",
       "1   tall   6   6   4  2.0   8   9   4   1   7   1\n",
       "2   tall   2   2   4  NaN  10   2   8   8   6   5\n",
       "3  short   8   5   3  NaN   2   5   6   5   6   6\n",
       "4   tall   2   5   5  NaN   3  10  10   1   8   4\n",
       "5   tall   7   2   9  NaN  10   1  10   4   4   1\n",
       "6   tall   4  10   9  8.0   9   1  10   2   1  10\n",
       "7  short   2   6   3  9.0   1   9  10   1   4   6\n",
       "8  short   6   4   5  NaN   5   8   7   8   7   5\n",
       "9   tall   4   1  10  8.0   8   9   7  10   6   6"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select\n",
    "\n",
    "We can use `select()` to grab specific columns from a Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| x0|\n",
      "+---+\n",
      "|  1|\n",
      "|  6|\n",
      "|  2|\n",
      "|  8|\n",
      "|  2|\n",
      "|  7|\n",
      "|  4|\n",
      "|  2|\n",
      "|  6|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.select('x0').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to grab multiple columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| x0| x1|\n",
      "+---+---+\n",
      "|  1|  2|\n",
      "|  6|  6|\n",
      "|  2|  2|\n",
      "|  8|  5|\n",
      "|  2|  5|\n",
      "|  7|  2|\n",
      "|  4| 10|\n",
      "|  2|  6|\n",
      "|  6|  4|\n",
      "|  4|  1|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.select('x0', 'x1').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also select specific columns as follows using a `Column` object. You will see both variants in the wild; one with a list of column names, and the one below referencing the `Column` itself (from the Data Frame). When would you want to use the literal column name versus the object `Column`? Look below. When we need to transform the values in the column inline, we have to use the `Column` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| x0| x1|\n",
      "+---+---+\n",
      "|  1|  2|\n",
      "|  6|  6|\n",
      "|  2|  2|\n",
      "|  8|  5|\n",
      "|  2|  5|\n",
      "|  7|  2|\n",
      "|  4| 10|\n",
      "|  2|  6|\n",
      "|  6|  4|\n",
      "|  4|  1|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.select(sdf['x0'], sdf['x1']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even modify values that we are retrieving. Below, we multiply the first column we want by two and the second column we want by three. Observe the column names. Yuck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|(x0 * 2)|(x1 * 3)|\n",
      "+--------+--------+\n",
      "|       2|       6|\n",
      "|      12|      18|\n",
      "|       4|       6|\n",
      "|      16|      15|\n",
      "|       4|      15|\n",
      "|      14|       6|\n",
      "|       8|      30|\n",
      "|       4|      18|\n",
      "|      12|      12|\n",
      "|       8|       3|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.select(sdf['x0'] * 2, sdf['x1'] * 3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fix the column names with `alias()`. Uh-oh! Notice how the parentheses are creeping in? Is this style of coding for `what` we are doing clear in intention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| y0| y1|\n",
      "+---+---+\n",
      "|  2|  6|\n",
      "| 12| 18|\n",
      "|  4|  6|\n",
      "| 16| 15|\n",
      "|  4| 15|\n",
      "| 14|  6|\n",
      "|  8| 30|\n",
      "|  4| 18|\n",
      "| 12| 12|\n",
      "|  8|  3|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.select((sdf['x0'] * 2).alias('y0'), (sdf['x1'] * 3).alias('y1')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I suppose a little formatting might help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| y0| y1|\n",
      "+---+---+\n",
      "|  2|  6|\n",
      "| 12| 18|\n",
      "|  4|  6|\n",
      "| 16| 15|\n",
      "|  4| 15|\n",
      "| 14|  6|\n",
      "|  8| 30|\n",
      "|  4| 18|\n",
      "| 12| 12|\n",
      "|  8|  3|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.select(\n",
    "    (sdf['x0'] * 2).alias('y0'), \n",
    "    (sdf['x1'] * 3).alias('y1'))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also apply boolean expressions with `select()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|(x0 > 5)|\n",
      "+--------+\n",
      "|   false|\n",
      "|    true|\n",
      "|   false|\n",
      "|    true|\n",
      "|   false|\n",
      "|    true|\n",
      "|   false|\n",
      "|   false|\n",
      "|    true|\n",
      "|   false|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.select(sdf['x0'] > 5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we get distinct values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| x0|\n",
      "+---+\n",
      "|  7|\n",
      "|  6|\n",
      "|  1|\n",
      "|  8|\n",
      "|  2|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.select('x0').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a set difference operation with `subtract()` as follows. We use `distinct()` to enforce uniqueness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| x0|\n",
      "+---+\n",
      "|  7|\n",
      "|  8|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x0 = sdf.select('x0')\n",
    "x1 = sdf.select('x1')\n",
    "diff = x0.subtract(x1)\n",
    "diff.distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+---+---+---+---+---+---+---+---+-------------+\n",
      "|height| x0| x1| x2| x3| x4| x5| x6| x7| x8| x9|height_truthy|\n",
      "+------+---+---+---+---+---+---+---+---+---+---+-------------+\n",
      "| short|  1|  2|  4|NaN|  5|  3|  2|  3|  8|  2|        false|\n",
      "|  tall|  6|  6|  4|2.0|  8|  9|  4|  1|  7|  1|         true|\n",
      "|  tall|  2|  2|  4|NaN| 10|  2|  8|  8|  6|  5|         true|\n",
      "| short|  8|  5|  3|NaN|  2|  5|  6|  5|  6|  6|        false|\n",
      "|  tall|  2|  5|  5|NaN|  3| 10| 10|  1|  8|  4|         true|\n",
      "|  tall|  7|  2|  9|NaN| 10|  1| 10|  4|  4|  1|         true|\n",
      "|  tall|  4| 10|  9|8.0|  9|  1| 10|  2|  1| 10|         true|\n",
      "| short|  2|  6|  3|9.0|  1|  9| 10|  1|  4|  6|        false|\n",
      "| short|  6|  4|  5|NaN|  5|  8|  7|  8|  7|  5|        false|\n",
      "|  tall|  4|  1| 10|8.0|  8|  9|  7| 10|  6|  6|         true|\n",
      "+------+---+---+---+---+---+---+---+---+---+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.withColumn('height_truthy', sdf['height'] == 'tall').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping columns is achieved with `drop()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['height',\n",
       " 'x0',\n",
       " 'x1',\n",
       " 'x2',\n",
       " 'x3',\n",
       " 'x4',\n",
       " 'x5',\n",
       " 'x6',\n",
       " 'x7',\n",
       " 'x8',\n",
       " 'x9',\n",
       " 'height_truthy']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.withColumn('height_truthy', sdf['height'] == 'tall').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['height', 'x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.withColumn('height_truthy', sdf['height'] == 'tall').drop('height_truthy').columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "\n",
    "How do we filter records in a Spark DataFrame? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+---+---+---+---+---+---+---+---+\n",
      "|height| x0| x1| x2| x3| x4| x5| x6| x7| x8| x9|\n",
      "+------+---+---+---+---+---+---+---+---+---+---+\n",
      "|  tall|  6|  6|  4|2.0|  8|  9|  4|  1|  7|  1|\n",
      "| short|  8|  5|  3|NaN|  2|  5|  6|  5|  6|  6|\n",
      "|  tall|  7|  2|  9|NaN| 10|  1| 10|  4|  4|  1|\n",
      "| short|  6|  4|  5|NaN|  5|  8|  7|  8|  7|  5|\n",
      "+------+---+---+---+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.filter(sdf['x0'] > 5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple filters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+---+---+---+---+---+---+---+---+\n",
      "|height| x0| x1| x2| x3| x4| x5| x6| x7| x8| x9|\n",
      "+------+---+---+---+---+---+---+---+---+---+---+\n",
      "|  tall|  6|  6|  4|2.0|  8|  9|  4|  1|  7|  1|\n",
      "+------+---+---+---+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.filter(sdf['x0'] > 5).filter(sdf['x1'] > 5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about dropping duplicates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| x0|\n",
      "+---+\n",
      "|  7|\n",
      "|  6|\n",
      "|  1|\n",
      "|  8|\n",
      "|  2|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.select('x0').dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordering\n",
    "\n",
    "How do we order the records?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+---+---+---+---+---+---+---+---+\n",
      "|height| x0| x1| x2| x3| x4| x5| x6| x7| x8| x9|\n",
      "+------+---+---+---+---+---+---+---+---+---+---+\n",
      "| short|  1|  2|  4|NaN|  5|  3|  2|  3|  8|  2|\n",
      "| short|  2|  6|  3|9.0|  1|  9| 10|  1|  4|  6|\n",
      "|  tall|  2|  5|  5|NaN|  3| 10| 10|  1|  8|  4|\n",
      "|  tall|  2|  2|  4|NaN| 10|  2|  8|  8|  6|  5|\n",
      "|  tall|  4| 10|  9|8.0|  9|  1| 10|  2|  1| 10|\n",
      "|  tall|  4|  1| 10|8.0|  8|  9|  7| 10|  6|  6|\n",
      "|  tall|  6|  6|  4|2.0|  8|  9|  4|  1|  7|  1|\n",
      "| short|  6|  4|  5|NaN|  5|  8|  7|  8|  7|  5|\n",
      "|  tall|  7|  2|  9|NaN| 10|  1| 10|  4|  4|  1|\n",
      "| short|  8|  5|  3|NaN|  2|  5|  6|  5|  6|  6|\n",
      "+------+---+---+---+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.orderBy('x0').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And ordering by multiple columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+---+---+---+---+---+---+---+---+\n",
      "|height| x0| x1| x2| x3| x4| x5| x6| x7| x8| x9|\n",
      "+------+---+---+---+---+---+---+---+---+---+---+\n",
      "| short|  1|  2|  4|NaN|  5|  3|  2|  3|  8|  2|\n",
      "| short|  2|  6|  3|9.0|  1|  9| 10|  1|  4|  6|\n",
      "| short|  6|  4|  5|NaN|  5|  8|  7|  8|  7|  5|\n",
      "| short|  8|  5|  3|NaN|  2|  5|  6|  5|  6|  6|\n",
      "|  tall|  2|  5|  5|NaN|  3| 10| 10|  1|  8|  4|\n",
      "|  tall|  2|  2|  4|NaN| 10|  2|  8|  8|  6|  5|\n",
      "|  tall|  4|  1| 10|8.0|  8|  9|  7| 10|  6|  6|\n",
      "|  tall|  4| 10|  9|8.0|  9|  1| 10|  2|  1| 10|\n",
      "|  tall|  6|  6|  4|2.0|  8|  9|  4|  1|  7|  1|\n",
      "|  tall|  7|  2|  9|NaN| 10|  1| 10|  4|  4|  1|\n",
      "+------+---+---+---+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.orderBy('height', 'x0').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And ordering descendingly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+---+---+---+---+---+---+---+---+\n",
      "|height| x0| x1| x2| x3| x4| x5| x6| x7| x8| x9|\n",
      "+------+---+---+---+---+---+---+---+---+---+---+\n",
      "|  tall|  7|  2|  9|NaN| 10|  1| 10|  4|  4|  1|\n",
      "|  tall|  6|  6|  4|2.0|  8|  9|  4|  1|  7|  1|\n",
      "|  tall|  4|  1| 10|8.0|  8|  9|  7| 10|  6|  6|\n",
      "|  tall|  4| 10|  9|8.0|  9|  1| 10|  2|  1| 10|\n",
      "|  tall|  2|  5|  5|NaN|  3| 10| 10|  1|  8|  4|\n",
      "|  tall|  2|  2|  4|NaN| 10|  2|  8|  8|  6|  5|\n",
      "| short|  8|  5|  3|NaN|  2|  5|  6|  5|  6|  6|\n",
      "| short|  6|  4|  5|NaN|  5|  8|  7|  8|  7|  5|\n",
      "| short|  2|  6|  3|9.0|  1|  9| 10|  1|  4|  6|\n",
      "| short|  1|  2|  4|NaN|  5|  3|  2|  3|  8|  2|\n",
      "+------+---+---+---+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.orderBy(sdf['height'].desc(), sdf['x0'].desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values\n",
    "\n",
    "How do we handle missing values? First, we can drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+---+---+---+---+---+---+---+---+\n",
      "|height| x0| x1| x2| x3| x4| x5| x6| x7| x8| x9|\n",
      "+------+---+---+---+---+---+---+---+---+---+---+\n",
      "|  tall|  6|  6|  4|2.0|  8|  9|  4|  1|  7|  1|\n",
      "|  tall|  4| 10|  9|8.0|  9|  1| 10|  2|  1| 10|\n",
      "| short|  2|  6|  3|9.0|  1|  9| 10|  1|  4|  6|\n",
      "|  tall|  4|  1| 10|8.0|  8|  9|  7| 10|  6|  6|\n",
      "+------+---+---+---+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.dropna().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or maybe we want to set missing values to zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+---+----+---+---+---+---+---+---+\n",
      "|height| x0| x1| x2|  x3| x4| x5| x6| x7| x8| x9|\n",
      "+------+---+---+---+----+---+---+---+---+---+---+\n",
      "| short|  1|  2|  4|-1.0|  5|  3|  2|  3|  8|  2|\n",
      "|  tall|  6|  6|  4| 2.0|  8|  9|  4|  1|  7|  1|\n",
      "|  tall|  2|  2|  4|-1.0| 10|  2|  8|  8|  6|  5|\n",
      "| short|  8|  5|  3|-1.0|  2|  5|  6|  5|  6|  6|\n",
      "|  tall|  2|  5|  5|-1.0|  3| 10| 10|  1|  8|  4|\n",
      "|  tall|  7|  2|  9|-1.0| 10|  1| 10|  4|  4|  1|\n",
      "|  tall|  4| 10|  9| 8.0|  9|  1| 10|  2|  1| 10|\n",
      "| short|  2|  6|  3| 9.0|  1|  9| 10|  1|  4|  6|\n",
      "| short|  6|  4|  5|-1.0|  5|  8|  7|  8|  7|  5|\n",
      "|  tall|  4|  1| 10| 8.0|  8|  9|  7| 10|  6|  6|\n",
      "+------+---+---+---+----+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.fillna(-1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group by\n",
    "\n",
    "How do we do grouping?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|height|count|\n",
      "+------+-----+\n",
      "|  tall|    6|\n",
      "| short|    4|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.groupBy('height').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do aggregations `agg()` after a group-by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+\n",
      "|height|          avg(x0)|\n",
      "+------+-----------------+\n",
      "|  tall|4.166666666666667|\n",
      "| short|             4.25|\n",
      "+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.groupBy('height').agg({'x0': 'mean'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple aggregations over different columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+-----------------+\n",
      "|height|          avg(x0)|          avg(x1)|\n",
      "+------+-----------------+-----------------+\n",
      "|  tall|4.166666666666667|4.333333333333333|\n",
      "| short|             4.25|             4.25|\n",
      "+------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.groupBy('height').agg({'x0': 'mean', 'x1': 'mean'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh-uh, it seems if we want multiple aggregation over the same column, the results will not compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+\n",
      "|height|       stddev(x0)|\n",
      "+------+-----------------+\n",
      "|  tall|2.041241452319315|\n",
      "| short|3.304037933599835|\n",
      "+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.groupBy('height').agg({'x0': 'mean', 'x0': 'stddev'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some group functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+\n",
      "|avg(x0)|  stddev_samp(x0)|count(DISTINCT x0)|\n",
      "+-------+-----------------+------------------+\n",
      "|    4.2|2.440400695696417|                 6|\n",
      "+-------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct, avg, stddev\n",
    "\n",
    "sdf.select(avg('x0'), stddev('x0'), countDistinct('x0')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-tabulation\n",
    "\n",
    "If we wanted to do cross-tabulation, we need to use `crosstab()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+---+---+---+---+---+\n",
      "|height_x1|  1| 10|  2|  4|  5|  6|\n",
      "+---------+---+---+---+---+---+---+\n",
      "|    short|  0|  0|  1|  1|  1|  1|\n",
      "|     tall|  1|  1|  2|  0|  1|  1|\n",
      "+---------+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.crosstab('height', 'x1').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics\n",
    "\n",
    "I want statistics. Ugh, the standard deviation has too much precision. How can we fix the precision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+\n",
      "|summary|               x0|                x1|\n",
      "+-------+-----------------+------------------+\n",
      "|  count|               10|                10|\n",
      "|   mean|              4.2|               4.3|\n",
      "| stddev|2.440400695696417|2.7100635498903793|\n",
      "|    min|                1|                 1|\n",
      "|    max|                8|                10|\n",
      "+-------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.describe('x0', 'x1').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With some coding gymnastics, we need to cast the columns `x0` and `x1` to `DoubleType` and then use `format_number()` to specify the precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+\n",
      "|summary|   x0|   x1|\n",
      "+-------+-----+-----+\n",
      "|  count|10.00|10.00|\n",
      "|   mean| 4.20| 4.30|\n",
      "| stddev| 2.44| 2.71|\n",
      "|    min| 1.00| 1.00|\n",
      "|    max| 8.00|10.00|\n",
      "+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import format_number, col\n",
    "\n",
    "sdf.describe('x0', 'x1')\\\n",
    "    .withColumn('x0', col('x0').cast(DoubleType()))\\\n",
    "    .withColumn('x1', col('x1').cast(DoubleType()))\\\n",
    "    .select('summary', format_number('x0', 2).alias('x0'), format_number('x1', 2).alias('x1'))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another way using `selectExpr()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+\n",
      "|summary|   x0|   x1|\n",
      "+-------+-----+-----+\n",
      "|  count|10.00|10.00|\n",
      "|   mean| 4.20| 4.30|\n",
      "| stddev| 2.44| 2.71|\n",
      "|    min| 1.00| 1.00|\n",
      "|    max| 8.00|10.00|\n",
      "+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.describe('x0', 'x1')\\\n",
    "    .selectExpr('summary', 'cast(x0 as double) as x0', 'cast(x1 as double) as x1')\\\n",
    "    .select(\n",
    "        'summary', \n",
    "        format_number('x0', 2).alias('x0'), \n",
    "        format_number('x1', 2).alias('x1'))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "\n",
    "We can also sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+---+---+---+---+---+---+---+---+\n",
      "|height| x0| x1| x2| x3| x4| x5| x6| x7| x8| x9|\n",
      "+------+---+---+---+---+---+---+---+---+---+---+\n",
      "|  tall|  2|  2|  4|NaN| 10|  2|  8|  8|  6|  5|\n",
      "|  tall|  2|  5|  5|NaN|  3| 10| 10|  1|  8|  4|\n",
      "+------+---+---+---+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.sample(True, 0.5, 37).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User defined function (UDF)\n",
    "\n",
    "If you have complicated logic to transform a column, you can use `User-Defined Functions` or `UDFs`. To create a UDF, define the function that will do the transformation first. Below, we define `times_two()` to take in a number input and return that number times two. The second thing you need to do is create the UDF using `udf()`, which requires two arguments:\n",
    "\n",
    "* the function that will do the transform \n",
    "* the return type\n",
    "\n",
    "The return type comes from the `pyspark.sql.types` module. Finally, you can apply your UDF as an argument to `select()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| x0|times_two|\n",
      "+---+---------+\n",
      "|  1|        2|\n",
      "|  6|       12|\n",
      "|  2|        4|\n",
      "|  8|       16|\n",
      "|  2|        4|\n",
      "|  7|       14|\n",
      "|  4|        8|\n",
      "|  2|        4|\n",
      "|  6|       12|\n",
      "|  4|        8|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def times_two(num):\n",
    "    return num * 2\n",
    "\n",
    "times_two_udf = udf(times_two, IntegerType())\n",
    "sdf.select('x0', times_two_udf('x0').alias('times_two')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A UDF can also accept multiple arguments. Here's an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------+\n",
      "| x0| x1|add_them|\n",
      "+---+---+--------+\n",
      "|  1|  2|       3|\n",
      "|  6|  6|      12|\n",
      "|  2|  2|       4|\n",
      "|  8|  5|      13|\n",
      "|  2|  5|       7|\n",
      "|  7|  2|       9|\n",
      "|  4| 10|      14|\n",
      "|  2|  6|       8|\n",
      "|  6|  4|      10|\n",
      "|  4|  1|       5|\n",
      "+---+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def add_them(a, b):\n",
    "    return a + b\n",
    "\n",
    "add_them_udf = udf(add_them, IntegerType())\n",
    "sdf.select('x0', 'x1', add_them_udf('x0', 'x1').alias('add_them')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User defined function (UDF) with annotation\n",
    "\n",
    "It's probably easiest to use the `@udf` decorator on a function. Note that the `@udf` decorator is parameterized; we have to specify the return type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| x0|times_three|\n",
      "+---+-----------+\n",
      "|  1|          3|\n",
      "|  6|         18|\n",
      "|  2|          6|\n",
      "|  8|         24|\n",
      "|  2|          6|\n",
      "|  7|         21|\n",
      "|  4|         12|\n",
      "|  2|          6|\n",
      "|  6|         18|\n",
      "|  4|         12|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "@udf('int')\n",
    "def times_three(num):\n",
    "    return num * 3\n",
    "\n",
    "sdf.select('x0', times_three('x0').alias('times_three')).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
