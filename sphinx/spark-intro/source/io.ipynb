{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input/Output\n",
    "\n",
    "It's very important to read and write data. Let's see how to read and write data in Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Here's some mock data we will create and use to illustrate IO operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from random import randint, choice\n",
    "\n",
    "def get_record(idx, n_cols):\n",
    "    gender = choice(['male', 'female'])\n",
    "    data = [idx, gender] + [randint(1, 100) for _ in range(n_cols)]\n",
    "    return tuple(data)\n",
    "\n",
    "n_cols = 10\n",
    "n_rows = 10\n",
    "\n",
    "data = [get_record(i, n_cols) for i, r in enumerate(range(n_rows))]\n",
    "columns = ['id', 'gender'] + [f'x{i}' for i in range(n_cols)]\n",
    "\n",
    "df = sqlContext.createDataFrame(pd.DataFrame(data, columns=columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+---+---+---+---+---+---+---+---+---+\n",
      "| id|gender| x0| x1| x2| x3| x4| x5| x6| x7| x8| x9|\n",
      "+---+------+---+---+---+---+---+---+---+---+---+---+\n",
      "|  0|female| 46| 47|  5| 89| 68| 10| 28| 85|  6| 79|\n",
      "|  1|  male| 61| 48| 88| 27| 86| 45| 29| 11| 20| 35|\n",
      "|  2|female|  1| 68|  3| 62| 53| 61| 97| 28| 35| 15|\n",
      "|  3|female| 55| 87| 51|100| 17| 41|  6| 41| 45| 56|\n",
      "|  4|female| 12| 73|  7| 58| 97| 34| 86| 68| 86| 14|\n",
      "|  5|female| 86| 70| 57| 74|  8| 11| 84| 42| 93| 16|\n",
      "|  6|female| 64| 60| 50| 28| 27| 69| 42| 58| 60| 11|\n",
      "|  7|female| 36| 97| 58| 92| 26| 74| 76| 17| 40| 42|\n",
      "|  8|  male| 85|  4| 50| 71| 72| 40| 88| 55| 30| 52|\n",
      "|  9|female| 16| 94| 42| 27| 78| 94| 37| 91| 87| 53|\n",
      "+---+------+---+---+---+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- x0: long (nullable = true)\n",
      " |-- x1: long (nullable = true)\n",
      " |-- x2: long (nullable = true)\n",
      " |-- x3: long (nullable = true)\n",
      " |-- x4: long (nullable = true)\n",
      " |-- x5: long (nullable = true)\n",
      " |-- x6: long (nullable = true)\n",
      " |-- x7: long (nullable = true)\n",
      " |-- x8: long (nullable = true)\n",
      " |-- x9: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV\n",
    "\n",
    "When we want to write a CSV file, we have to specify the format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write\\\n",
    "    .format('com.databricks.spark.csv')\\\n",
    "    .mode('overwrite')\\\n",
    "    .option('header', 'true')\\\n",
    "    .save('/user/root/data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that multiple CSV files are written as data will be written in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/user/root/data.csv/_SUCCESS\n",
      "/user/root/data.csv/part-00000-b435899e-e687-467a-9182-b6f7753db8b0-c000.csv\n",
      "/user/root/data.csv/part-00001-b435899e-e687-467a-9182-b6f7753db8b0-c000.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "hdfs dfs -ls /user/root/data.csv | awk '{print $8}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As one CSV file\n",
    "\n",
    "If we wanted to write to only 1 file, we need to create one partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.repartition(1).write\\\n",
    "    .format('com.databricks.spark.csv')\\\n",
    "    .mode('overwrite')\\\n",
    "    .option('header', 'true')\\\n",
    "    .save('/user/root/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/user/root/data.csv/_SUCCESS\n",
      "/user/root/data.csv/part-00000-c15d5a89-dcf8-4815-b075-e5a1af1cd2e4-c000.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "hdfs dfs -ls /user/root/data.csv | awk '{print $8}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON\n",
    "\n",
    "Writing data as JSON is accomplished by specifying the JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write\\\n",
    "    .format('json')\\\n",
    "    .mode('overwrite')\\\n",
    "    .save('/user/root/data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/user/root/data.json/_SUCCESS\n",
      "/user/root/data.json/part-00000-b7e4462b-99cb-4cba-b430-2738cd8f8f35-c000.json\n",
      "/user/root/data.json/part-00001-b7e4462b-99cb-4cba-b430-2738cd8f8f35-c000.json\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "hdfs dfs -ls /user/root/data.json | awk '{print $8}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet\n",
    "\n",
    "Parquet files are also easy to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write\\\n",
    "    .format('parquet')\\\n",
    "    .mode('overwrite')\\\n",
    "    .save('/user/root/data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/user/root/data.parquet/_SUCCESS\n",
      "/user/root/data.parquet/part-00000-cd6f0ab1-b720-4e23-9a85-1b11eec25ff3-c000.snappy.parquet\n",
      "/user/root/data.parquet/part-00001-cd6f0ab1-b720-4e23-9a85-1b11eec25ff3-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "hdfs dfs -ls /user/root/data.parquet | awk '{print $8}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet with partitions\n",
    "\n",
    "If we want to create parititioned Parquet files, we need to specify which field to parition by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write\\\n",
    "    .format('parquet')\\\n",
    "    .mode('overwrite')\\\n",
    "    .partitionBy('gender')\\\n",
    "    .save('/user/root/data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/user/root/data.parquet/_SUCCESS\n",
      "/user/root/data.parquet/gender=female\n",
      "/user/root/data.parquet/gender=male\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "hdfs dfs -ls /user/root/data.parquet | awk '{print $8}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/user/root/data.parquet/gender=female/part-00000-4db76505-3e45-41e1-90cb-557bf85c92df.c000.snappy.parquet\n",
      "/user/root/data.parquet/gender=female/part-00001-4db76505-3e45-41e1-90cb-557bf85c92df.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "hdfs dfs -ls /user/root/data.parquet/gender=female | awk '{print $8}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/user/root/data.parquet/gender=male/part-00000-4db76505-3e45-41e1-90cb-557bf85c92df.c000.snappy.parquet\n",
      "/user/root/data.parquet/gender=male/part-00001-4db76505-3e45-41e1-90cb-557bf85c92df.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "hdfs dfs -ls /user/root/data.parquet/gender=male | awk '{print $8}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORC\n",
    "\n",
    "ORC files are created by specifying the ORC file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write\\\n",
    "    .format('orc')\\\n",
    "    .mode('overwrite')\\\n",
    "    .save('/user/root/data.orc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/user/root/data.orc/_SUCCESS\n",
      "/user/root/data.orc/part-00000-c0736144-12b8-4248-ab6b-9b1e18de48b8-c000.snappy.orc\n",
      "/user/root/data.orc/part-00001-c0736144-12b8-4248-ab6b-9b1e18de48b8-c000.snappy.orc\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "hdfs dfs -ls /user/root/data.orc | awk '{print $8}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whole text file\n",
    "\n",
    "The function `wholeTextFiles()` will read all the contents of files into a RDD of 2-tuple, where the first element is path of the file and the second element is the entire content of the file. See how we can specify wildcards `*` to read more than one file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item[0] = hdfs://localhost/data.csv\n",
      "item[1][0:90] = x0,x1,x2,x3,x4,x5,x6,x7,x8,x9\n",
      "14,22,25,63,47,52,13,14,23,27\n",
      "35,80,38,28,73,69,21,16,76,53\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pair_rdd = sc.wholeTextFiles('hdfs://localhost/*.csv')\n",
    "\n",
    "item = pair_rdd.collect()[0]\n",
    "\n",
    "print(f'item[0] = {item[0]}')\n",
    "print(f'item[1][0:90] = {item[1][0:90]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text file by lines\n",
    "\n",
    "If we wanted to read a file line-by-line, then use `textFile()`. A RDD of strings will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x0,x1,x2,x3,x4,x5,x6,x7,x8,x9',\n",
       " '14,22,25,63,47,52,13,14,23,27',\n",
       " '35,80,38,28,73,69,21,16,76,53',\n",
       " '46,37,46,55,78,68,61,62,81,82',\n",
       " '19,12,45,50,71,63,94,7,10,77']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.textFile('hdfs://localhost/data.csv')\n",
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV\n",
    "\n",
    "We can also read CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+---+---+---+---+---+---+---+---+---+\n",
      "| id|gender| x0| x1| x2| x3| x4| x5| x6| x7| x8| x9|\n",
      "+---+------+---+---+---+---+---+---+---+---+---+---+\n",
      "|  0|female| 46| 47|  5| 89| 68| 10| 28| 85|  6| 79|\n",
      "|  1|  male| 61| 48| 88| 27| 86| 45| 29| 11| 20| 35|\n",
      "|  2|female|  1| 68|  3| 62| 53| 61| 97| 28| 35| 15|\n",
      "|  3|female| 55| 87| 51|100| 17| 41|  6| 41| 45| 56|\n",
      "|  4|female| 12| 73|  7| 58| 97| 34| 86| 68| 86| 14|\n",
      "|  5|female| 86| 70| 57| 74|  8| 11| 84| 42| 93| 16|\n",
      "|  6|female| 64| 60| 50| 28| 27| 69| 42| 58| 60| 11|\n",
      "|  7|female| 36| 97| 58| 92| 26| 74| 76| 17| 40| 42|\n",
      "|  8|  male| 85|  4| 50| 71| 72| 40| 88| 55| 30| 52|\n",
      "|  9|female| 16| 94| 42| 27| 78| 94| 37| 91| 87| 53|\n",
      "+---+------+---+---+---+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format('csv')\\\n",
    "    .option('header', 'true')\\\n",
    "    .option('inferSchema', 'true')\\\n",
    "    .load('/user/root/data.csv')\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON\n",
    "\n",
    "JSON files are read as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|gender| id| x0| x1| x2| x3| x4| x5| x6| x7| x8| x9|\n",
      "+------+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|female|  5| 86| 70| 57| 74|  8| 11| 84| 42| 93| 16|\n",
      "|female|  6| 64| 60| 50| 28| 27| 69| 42| 58| 60| 11|\n",
      "|female|  7| 36| 97| 58| 92| 26| 74| 76| 17| 40| 42|\n",
      "|  male|  8| 85|  4| 50| 71| 72| 40| 88| 55| 30| 52|\n",
      "|female|  9| 16| 94| 42| 27| 78| 94| 37| 91| 87| 53|\n",
      "|female|  0| 46| 47|  5| 89| 68| 10| 28| 85|  6| 79|\n",
      "|  male|  1| 61| 48| 88| 27| 86| 45| 29| 11| 20| 35|\n",
      "|female|  2|  1| 68|  3| 62| 53| 61| 97| 28| 35| 15|\n",
      "|female|  3| 55| 87| 51|100| 17| 41|  6| 41| 45| 56|\n",
      "|female|  4| 12| 73|  7| 58| 97| 34| 86| 68| 86| 14|\n",
      "+------+---+---+---+---+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format('json')\\\n",
    "    .option('inferSchema', 'true')\\\n",
    "    .load('/user/root/data.json')\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet\n",
    "\n",
    "Parquet files are easy to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+---+---+---+---+---+------+\n",
      "| id| x0| x1| x2| x3| x4| x5| x6| x7| x8| x9|gender|\n",
      "+---+---+---+---+---+---+---+---+---+---+---+------+\n",
      "|  5| 86| 70| 57| 74|  8| 11| 84| 42| 93| 16|female|\n",
      "|  6| 64| 60| 50| 28| 27| 69| 42| 58| 60| 11|female|\n",
      "|  7| 36| 97| 58| 92| 26| 74| 76| 17| 40| 42|female|\n",
      "|  9| 16| 94| 42| 27| 78| 94| 37| 91| 87| 53|female|\n",
      "|  0| 46| 47|  5| 89| 68| 10| 28| 85|  6| 79|female|\n",
      "|  2|  1| 68|  3| 62| 53| 61| 97| 28| 35| 15|female|\n",
      "|  3| 55| 87| 51|100| 17| 41|  6| 41| 45| 56|female|\n",
      "|  4| 12| 73|  7| 58| 97| 34| 86| 68| 86| 14|female|\n",
      "|  1| 61| 48| 88| 27| 86| 45| 29| 11| 20| 35|  male|\n",
      "|  8| 85|  4| 50| 71| 72| 40| 88| 55| 30| 52|  male|\n",
      "+---+---+---+---+---+---+---+---+---+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet('/user/root/data.parquet').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORC\n",
    "\n",
    "No problems as well reading ORC files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+---+---+---+---+---+---+---+---+---+\n",
      "| id|gender| x0| x1| x2| x3| x4| x5| x6| x7| x8| x9|\n",
      "+---+------+---+---+---+---+---+---+---+---+---+---+\n",
      "|  5|female| 86| 70| 57| 74|  8| 11| 84| 42| 93| 16|\n",
      "|  6|female| 64| 60| 50| 28| 27| 69| 42| 58| 60| 11|\n",
      "|  7|female| 36| 97| 58| 92| 26| 74| 76| 17| 40| 42|\n",
      "|  8|  male| 85|  4| 50| 71| 72| 40| 88| 55| 30| 52|\n",
      "|  9|female| 16| 94| 42| 27| 78| 94| 37| 91| 87| 53|\n",
      "|  0|female| 46| 47|  5| 89| 68| 10| 28| 85|  6| 79|\n",
      "|  1|  male| 61| 48| 88| 27| 86| 45| 29| 11| 20| 35|\n",
      "|  2|female|  1| 68|  3| 62| 53| 61| 97| 28| 35| 15|\n",
      "|  3|female| 55| 87| 51|100| 17| 41|  6| 41| 45| 56|\n",
      "|  4|female| 12| 73|  7| 58| 97| 34| 86| 68| 86| 14|\n",
      "+---+------+---+---+---+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.orc('/user/root/data.orc').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
