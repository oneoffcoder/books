{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Stream, DStreams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-10-31 09:34:34\n",
      "-------------------------------------------\n",
      "('v', 1)\n",
      "('l', 1)\n",
      "('s', 1)\n",
      "('l', 1)\n",
      "('p', 1)\n",
      "('v', 1)\n",
      "('f', 1)\n",
      "('a', 1)\n",
      "('h', 1)\n",
      "('x', 1)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 09:34:35\n",
      "-------------------------------------------\n",
      "('s', 1)\n",
      "('z', 1)\n",
      "('h', 1)\n",
      "('c', 1)\n",
      "('e', 1)\n",
      "('u', 1)\n",
      "('v', 1)\n",
      "('c', 1)\n",
      "('h', 1)\n",
      "('b', 1)\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from random import choice\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint('/tmp')\n",
    "\n",
    "alphabets = list('abcdefghijklmnopqrstuvwxyz')\n",
    "input_data = [[choice(alphabets) for _ in range(100)] for _ in range(100)]\n",
    "rdd_queue = [ssc.sparkContext.parallelize(item) for item in input_data]\n",
    "\n",
    "stream = ssc.queueStream(rdd_queue).map(lambda word: (word, 1))\n",
    "stream.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flat map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-10-31 09:34:28\n",
      "-------------------------------------------\n",
      "(1, 6)\n",
      "(2, 10)\n",
      "(2, 6)\n",
      "(3, 8)\n",
      "(3, 3)\n",
      "(3, 1)\n",
      "(4, 4)\n",
      "(4, 8)\n",
      "(4, 7)\n",
      "(4, 9)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 09:34:29\n",
      "-------------------------------------------\n",
      "(1, 6)\n",
      "(2, 10)\n",
      "(2, 6)\n",
      "(3, 8)\n",
      "(3, 3)\n",
      "(3, 1)\n",
      "(4, 4)\n",
      "(4, 8)\n",
      "(4, 7)\n",
      "(4, 9)\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from random import randint\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint('/tmp')\n",
    "\n",
    "input_data = [[i for i in range(100)] for _ in range(100)]\n",
    "rdd_queue = [ssc.sparkContext.parallelize(item) for item in input_data]\n",
    "\n",
    "stream = ssc.queueStream(rdd_queue).flatMap(lambda num: [(num, randint(1, 10)) for _ in range(num)])\n",
    "stream.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-10-31 09:34:19\n",
      "-------------------------------------------\n",
      "54\n",
      "22\n",
      "88\n",
      "66\n",
      "78\n",
      "90\n",
      "22\n",
      "40\n",
      "2\n",
      "52\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 09:34:20\n",
      "-------------------------------------------\n",
      "52\n",
      "82\n",
      "16\n",
      "16\n",
      "38\n",
      "46\n",
      "12\n",
      "6\n",
      "74\n",
      "46\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from random import randint\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint('/tmp')\n",
    "\n",
    "input_data = [[randint(1, 100) for i in range(100)] for _ in range(100)]\n",
    "rdd_queue = [ssc.sparkContext.parallelize(item) for item in input_data]\n",
    "\n",
    "stream = ssc.queueStream(rdd_queue).filter(lambda num: num % 2 == 0)\n",
    "stream.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-10-31 09:34:10\n",
      "-------------------------------------------\n",
      "44\n",
      "91\n",
      "86\n",
      "98\n",
      "51\n",
      "47\n",
      "60\n",
      "74\n",
      "3\n",
      "96\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 09:34:11\n",
      "-------------------------------------------\n",
      "16\n",
      "86\n",
      "82\n",
      "93\n",
      "18\n",
      "88\n",
      "27\n",
      "43\n",
      "89\n",
      "82\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from random import randint\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint('/tmp')\n",
    "\n",
    "input_data = [[randint(1, 100) for i in range(100)] for _ in range(100)]\n",
    "rdd_queue = [ssc.sparkContext.parallelize(item) for item in input_data]\n",
    "\n",
    "stream = ssc.queueStream(rdd_queue).repartition(1)\n",
    "stream.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-10-31 09:33:58\n",
      "-------------------------------------------\n",
      "34\n",
      "60\n",
      "70\n",
      "18\n",
      "56\n",
      "72\n",
      "82\n",
      "62\n",
      "2\n",
      "44\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 09:33:59\n",
      "-------------------------------------------\n",
      "68\n",
      "78\n",
      "98\n",
      "50\n",
      "32\n",
      "48\n",
      "86\n",
      "20\n",
      "64\n",
      "50\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from random import randint\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint('/tmp')\n",
    "\n",
    "input_data1 = [[randint(1, 100) for i in range(100)] for _ in range(100)]\n",
    "input_data2 = [[randint(1, 100) for i in range(100)] for _ in range(100)]\n",
    "\n",
    "rdd_queue1 = [ssc.sparkContext.parallelize(item) for item in input_data1]\n",
    "rdd_queue2 = [ssc.sparkContext.parallelize(item) for item in input_data2]\n",
    "\n",
    "stream1 = ssc.queueStream(rdd_queue1).filter(lambda num: num % 2 == 0)\n",
    "stream2 = ssc.queueStream(rdd_queue2).filter(lambda num: num % 2 == 0)\n",
    "\n",
    "stream = stream1.union(stream2)\n",
    "stream.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-10-31 09:16:04\n",
      "-------------------------------------------\n",
      "11\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 09:16:05\n",
      "-------------------------------------------\n",
      "13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from random import randint\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint('/tmp')\n",
    "\n",
    "input_data = [[randint(1, 100) for _ in range(randint(1, 20))] for i in range(100)]\n",
    "rdd_queue = [ssc.sparkContext.parallelize(item) for item in input_data]\n",
    "\n",
    "stream = ssc.queueStream(rdd_queue).count()\n",
    "stream.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-10-31 09:16:51\n",
      "-------------------------------------------\n",
      "671\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 09:16:52\n",
      "-------------------------------------------\n",
      "421\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from random import randint\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint('/tmp')\n",
    "\n",
    "input_data = [[randint(1, 100) for _ in range(randint(1, 20))] for i in range(100)]\n",
    "rdd_queue = [ssc.sparkContext.parallelize(item) for item in input_data]\n",
    "\n",
    "stream = ssc.queueStream(rdd_queue).reduce(lambda a, b: a + b)\n",
    "stream.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count by value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-10-31 09:17:48\n",
      "-------------------------------------------\n",
      "(16, 1)\n",
      "(100, 1)\n",
      "(91, 1)\n",
      "(34, 1)\n",
      "(11, 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 09:17:49\n",
      "-------------------------------------------\n",
      "(51, 1)\n",
      "(16, 2)\n",
      "(52, 1)\n",
      "(17, 1)\n",
      "(54, 1)\n",
      "(30, 1)\n",
      "(79, 1)\n",
      "(44, 1)\n",
      "(59, 1)\n",
      "(95, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from random import randint\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint('/tmp')\n",
    "\n",
    "input_data = [[randint(1, 100) for _ in range(randint(1, 20))] for i in range(100)]\n",
    "rdd_queue = [ssc.sparkContext.parallelize(item) for item in input_data]\n",
    "\n",
    "stream = ssc.queueStream(rdd_queue).countByValue()\n",
    "stream.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce by key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-10-31 09:19:58\n",
      "-------------------------------------------\n",
      "(1, 301)\n",
      "(2, 260)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 09:19:59\n",
      "-------------------------------------------\n",
      "(1, 147)\n",
      "(2, 379)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from random import randint\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint('/tmp')\n",
    "\n",
    "input_data = [[(randint(1, 2), randint(1, 100)) for _ in range(randint(1, 20))] for i in range(100)]\n",
    "rdd_queue = [ssc.sparkContext.parallelize(item) for item in input_data]\n",
    "\n",
    "stream = ssc.queueStream(rdd_queue).reduceByKey(lambda a, b: a + b)\n",
    "stream.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-10-31 09:32:47\n",
      "-------------------------------------------\n",
      "(0, (1, 'b'))\n",
      "(0, (1, 'a'))\n",
      "(0, (1, 'b'))\n",
      "(0, (2, 'b'))\n",
      "(0, (2, 'a'))\n",
      "(0, (2, 'b'))\n",
      "(1, (1, 'b'))\n",
      "(1, (1, 'a'))\n",
      "(1, (2, 'b'))\n",
      "(1, (2, 'a'))\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 09:32:48\n",
      "-------------------------------------------\n",
      "(0, (1, 'a'))\n",
      "(0, (1, 'b'))\n",
      "(0, (1, 'a'))\n",
      "(0, (2, 'a'))\n",
      "(0, (2, 'b'))\n",
      "(0, (2, 'a'))\n",
      "(0, (2, 'a'))\n",
      "(0, (2, 'b'))\n",
      "(0, (2, 'a'))\n",
      "(1, (2, 'a'))\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from random import randint, choice\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint('/tmp')\n",
    "\n",
    "input_data1 = [[(choice([0, 1]), randint(1, 2)) for _ in range(5)] for _ in range(100)]\n",
    "input_data2 = [[(choice([0, 1]), choice(['a', 'b'])) for _ in range(5)] for _ in range(100)]\n",
    "\n",
    "rdd_queue1 = [ssc.sparkContext.parallelize(item) for item in input_data1]\n",
    "rdd_queue2 = [ssc.sparkContext.parallelize(item) for item in input_data2]\n",
    "\n",
    "counts1 = ssc.queueStream(rdd_queue1)\n",
    "counts2 = ssc.queueStream(rdd_queue2)\n",
    "\n",
    "stream = counts1.join(counts2)\n",
    "stream.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cogroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-10-31 09:32:32\n",
      "-------------------------------------------\n",
      "(0, (<pyspark.resultiterable.ResultIterable object at 0x7f1a30410ed0>, <pyspark.resultiterable.ResultIterable object at 0x7f1a30412050>))\n",
      "(1, (<pyspark.resultiterable.ResultIterable object at 0x7f1a30410d90>, <pyspark.resultiterable.ResultIterable object at 0x7f1a30410250>))\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 09:32:33\n",
      "-------------------------------------------\n",
      "(0, (<pyspark.resultiterable.ResultIterable object at 0x7f1a3040f650>, <pyspark.resultiterable.ResultIterable object at 0x7f1a3040f6d0>))\n",
      "(1, (<pyspark.resultiterable.ResultIterable object at 0x7f1a3040fd10>, <pyspark.resultiterable.ResultIterable object at 0x7f1a303fe8d0>))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from random import randint, choice\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint('/tmp')\n",
    "\n",
    "input_data1 = [[(choice([0, 1]), randint(1, 2)) for _ in range(5)] for _ in range(100)]\n",
    "input_data2 = [[(choice([0, 1]), choice(['a', 'b'])) for _ in range(5)] for _ in range(100)]\n",
    "\n",
    "rdd_queue1 = [ssc.sparkContext.parallelize(item) for item in input_data1]\n",
    "rdd_queue2 = [ssc.sparkContext.parallelize(item) for item in input_data2]\n",
    "\n",
    "counts1 = ssc.queueStream(rdd_queue1)\n",
    "counts2 = ssc.queueStream(rdd_queue2)\n",
    "\n",
    "stream = counts1.cogroup(counts2)\n",
    "stream.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-10-31 09:32:22\n",
      "-------------------------------------------\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "14\n",
      "16\n",
      "18\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 09:32:23\n",
      "-------------------------------------------\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "14\n",
      "16\n",
      "18\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from random import randint\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint('/tmp')\n",
    "\n",
    "input_data = [[i for i in range(100)] for _ in range(100)]\n",
    "rdd_queue = [ssc.sparkContext.parallelize(item) for item in input_data]\n",
    "\n",
    "stream = ssc.queueStream(rdd_queue).transform(lambda rdd: rdd.filter(lambda x: x % 2 == 0))\n",
    "stream.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
