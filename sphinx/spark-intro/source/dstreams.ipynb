{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Stream, DStreams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:24:14\n",
      "-------------------------------------------\n",
      "('s', 1)\n",
      "('c', 1)\n",
      "('c', 1)\n",
      "('g', 1)\n",
      "('h', 1)\n",
      "('z', 1)\n",
      "('x', 1)\n",
      "('u', 1)\n",
      "('x', 1)\n",
      "('j', 1)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:24:15\n",
      "-------------------------------------------\n",
      "('j', 1)\n",
      "('z', 1)\n",
      "('m', 1)\n",
      "('n', 1)\n",
      "('s', 1)\n",
      "('d', 1)\n",
      "('n', 1)\n",
      "('o', 1)\n",
      "('x', 1)\n",
      "('h', 1)\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from random import choice\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint('/tmp')\n",
    "\n",
    "alphabets = list('abcdefghijklmnopqrstuvwxyz')\n",
    "input_data = [[choice(alphabets) for _ in range(100)] for _ in range(100)]\n",
    "rdd_queue = [ssc.sparkContext.parallelize(item) for item in input_data]\n",
    "\n",
    "stream = ssc.queueStream(rdd_queue).map(lambda word: (word, 1))\n",
    "stream.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flat map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:24:17\n",
      "-------------------------------------------\n",
      "(1, 8)\n",
      "(2, 2)\n",
      "(2, 9)\n",
      "(3, 4)\n",
      "(3, 5)\n",
      "(3, 1)\n",
      "(4, 8)\n",
      "(4, 9)\n",
      "(4, 1)\n",
      "(4, 9)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:24:18\n",
      "-------------------------------------------\n",
      "(1, 8)\n",
      "(2, 2)\n",
      "(2, 9)\n",
      "(3, 4)\n",
      "(3, 5)\n",
      "(3, 1)\n",
      "(4, 8)\n",
      "(4, 9)\n",
      "(4, 1)\n",
      "(4, 9)\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from random import randint\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint('/tmp')\n",
    "\n",
    "input_data = [[i for i in range(100)] for _ in range(100)]\n",
    "rdd_queue = [ssc.sparkContext.parallelize(item) for item in input_data]\n",
    "\n",
    "stream = ssc.queueStream(rdd_queue).flatMap(lambda num: [(num, randint(1, 10)) for _ in range(num)])\n",
    "stream.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:24:19\n",
      "-------------------------------------------\n",
      "88\n",
      "16\n",
      "30\n",
      "8\n",
      "58\n",
      "68\n",
      "30\n",
      "80\n",
      "62\n",
      "86\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:24:20\n",
      "-------------------------------------------\n",
      "26\n",
      "54\n",
      "16\n",
      "12\n",
      "38\n",
      "22\n",
      "68\n",
      "48\n",
      "78\n",
      "20\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from random import randint\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint('/tmp')\n",
    "\n",
    "input_data = [[randint(1, 100) for i in range(100)] for _ in range(100)]\n",
    "rdd_queue = [ssc.sparkContext.parallelize(item) for item in input_data]\n",
    "\n",
    "stream = ssc.queueStream(rdd_queue).filter(lambda num: num % 2 == 0)\n",
    "stream.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:24:21\n",
      "-------------------------------------------\n",
      "30\n",
      "23\n",
      "32\n",
      "29\n",
      "60\n",
      "10\n",
      "73\n",
      "57\n",
      "92\n",
      "85\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:24:22\n",
      "-------------------------------------------\n",
      "95\n",
      "93\n",
      "13\n",
      "41\n",
      "60\n",
      "11\n",
      "24\n",
      "57\n",
      "67\n",
      "70\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from random import randint\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint('/tmp')\n",
    "\n",
    "input_data = [[randint(1, 100) for i in range(100)] for _ in range(100)]\n",
    "rdd_queue = [ssc.sparkContext.parallelize(item) for item in input_data]\n",
    "\n",
    "stream = ssc.queueStream(rdd_queue).repartition(1)\n",
    "stream.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:24:24\n",
      "-------------------------------------------\n",
      "64\n",
      "56\n",
      "40\n",
      "26\n",
      "86\n",
      "92\n",
      "94\n",
      "70\n",
      "84\n",
      "96\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:24:25\n",
      "-------------------------------------------\n",
      "94\n",
      "42\n",
      "68\n",
      "74\n",
      "88\n",
      "96\n",
      "92\n",
      "80\n",
      "66\n",
      "2\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from random import randint\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint('/tmp')\n",
    "\n",
    "input_data1 = [[randint(1, 100) for i in range(100)] for _ in range(100)]\n",
    "input_data2 = [[randint(1, 100) for i in range(100)] for _ in range(100)]\n",
    "\n",
    "rdd_queue1 = [ssc.sparkContext.parallelize(item) for item in input_data1]\n",
    "rdd_queue2 = [ssc.sparkContext.parallelize(item) for item in input_data2]\n",
    "\n",
    "stream1 = ssc.queueStream(rdd_queue1).filter(lambda num: num % 2 == 0)\n",
    "stream2 = ssc.queueStream(rdd_queue2).filter(lambda num: num % 2 == 0)\n",
    "\n",
    "stream = stream1.union(stream2)\n",
    "stream.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:24:27\n",
      "-------------------------------------------\n",
      "6\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:24:28\n",
      "-------------------------------------------\n",
      "2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from random import randint\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint('/tmp')\n",
    "\n",
    "input_data = [[randint(1, 100) for _ in range(randint(1, 20))] for i in range(100)]\n",
    "rdd_queue = [ssc.sparkContext.parallelize(item) for item in input_data]\n",
    "\n",
    "stream = ssc.queueStream(rdd_queue).count()\n",
    "stream.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:24:30\n",
      "-------------------------------------------\n",
      "129\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:24:31\n",
      "-------------------------------------------\n",
      "572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from random import randint\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint('/tmp')\n",
    "\n",
    "input_data = [[randint(1, 100) for _ in range(randint(1, 20))] for i in range(100)]\n",
    "rdd_queue = [ssc.sparkContext.parallelize(item) for item in input_data]\n",
    "\n",
    "stream = ssc.queueStream(rdd_queue).reduce(lambda a, b: a + b)\n",
    "stream.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count by value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:24:33\n",
      "-------------------------------------------\n",
      "(57, 1)\n",
      "(75, 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:24:34\n",
      "-------------------------------------------\n",
      "(60, 2)\n",
      "(40, 1)\n",
      "(20, 1)\n",
      "(72, 1)\n",
      "(56, 1)\n",
      "(28, 1)\n",
      "(96, 1)\n",
      "(69, 1)\n",
      "(13, 1)\n",
      "(53, 1)\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from random import randint\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint('/tmp')\n",
    "\n",
    "input_data = [[randint(1, 100) for _ in range(randint(1, 20))] for i in range(100)]\n",
    "rdd_queue = [ssc.sparkContext.parallelize(item) for item in input_data]\n",
    "\n",
    "stream = ssc.queueStream(rdd_queue).countByValue()\n",
    "stream.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce by key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:24:36\n",
      "-------------------------------------------\n",
      "(1, 178)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:24:37\n",
      "-------------------------------------------\n",
      "(1, 104)\n",
      "(2, 79)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from random import randint\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint('/tmp')\n",
    "\n",
    "input_data = [[(randint(1, 2), randint(1, 100)) for _ in range(randint(1, 20))] for i in range(100)]\n",
    "rdd_queue = [ssc.sparkContext.parallelize(item) for item in input_data]\n",
    "\n",
    "stream = ssc.queueStream(rdd_queue).reduceByKey(lambda a, b: a + b)\n",
    "stream.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:24:39\n",
      "-------------------------------------------\n",
      "(0, (1, 'b'))\n",
      "(0, (1, 'a'))\n",
      "(1, (2, 'b'))\n",
      "(1, (2, 'a'))\n",
      "(1, (2, 'a'))\n",
      "(1, (2, 'b'))\n",
      "(1, (2, 'a'))\n",
      "(1, (2, 'a'))\n",
      "(1, (2, 'b'))\n",
      "(1, (2, 'a'))\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:24:40\n",
      "-------------------------------------------\n",
      "(0, (2, 'b'))\n",
      "(0, (2, 'a'))\n",
      "(1, (1, 'b'))\n",
      "(1, (1, 'b'))\n",
      "(1, (1, 'a'))\n",
      "(1, (1, 'b'))\n",
      "(1, (1, 'b'))\n",
      "(1, (1, 'a'))\n",
      "(1, (1, 'b'))\n",
      "(1, (1, 'b'))\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from random import randint, choice\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint('/tmp')\n",
    "\n",
    "input_data1 = [[(choice([0, 1]), randint(1, 2)) for _ in range(5)] for _ in range(100)]\n",
    "input_data2 = [[(choice([0, 1]), choice(['a', 'b'])) for _ in range(5)] for _ in range(100)]\n",
    "\n",
    "rdd_queue1 = [ssc.sparkContext.parallelize(item) for item in input_data1]\n",
    "rdd_queue2 = [ssc.sparkContext.parallelize(item) for item in input_data2]\n",
    "\n",
    "counts1 = ssc.queueStream(rdd_queue1)\n",
    "counts2 = ssc.queueStream(rdd_queue2)\n",
    "\n",
    "stream = counts1.join(counts2)\n",
    "stream.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cogroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:24:42\n",
      "-------------------------------------------\n",
      "(0, (<pyspark.resultiterable.ResultIterable object at 0x7f87990f9f90>, <pyspark.resultiterable.ResultIterable object at 0x7f87990f9a90>))\n",
      "(1, (<pyspark.resultiterable.ResultIterable object at 0x7f87990f9e10>, <pyspark.resultiterable.ResultIterable object at 0x7f87990f3d90>))\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:24:43\n",
      "-------------------------------------------\n",
      "(0, (<pyspark.resultiterable.ResultIterable object at 0x7f87990fc490>, <pyspark.resultiterable.ResultIterable object at 0x7f87990fc710>))\n",
      "(1, (<pyspark.resultiterable.ResultIterable object at 0x7f87990fc650>, <pyspark.resultiterable.ResultIterable object at 0x7f87990ece90>))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from random import randint, choice\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint('/tmp')\n",
    "\n",
    "input_data1 = [[(choice([0, 1]), randint(1, 2)) for _ in range(5)] for _ in range(100)]\n",
    "input_data2 = [[(choice([0, 1]), choice(['a', 'b'])) for _ in range(5)] for _ in range(100)]\n",
    "\n",
    "rdd_queue1 = [ssc.sparkContext.parallelize(item) for item in input_data1]\n",
    "rdd_queue2 = [ssc.sparkContext.parallelize(item) for item in input_data2]\n",
    "\n",
    "counts1 = ssc.queueStream(rdd_queue1)\n",
    "counts2 = ssc.queueStream(rdd_queue2)\n",
    "\n",
    "stream = counts1.cogroup(counts2)\n",
    "stream.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:24:44\n",
      "-------------------------------------------\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "14\n",
      "16\n",
      "18\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:24:45\n",
      "-------------------------------------------\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "14\n",
      "16\n",
      "18\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:24:46\n",
      "-------------------------------------------\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "14\n",
      "16\n",
      "18\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from random import randint\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint('/tmp')\n",
    "\n",
    "input_data = [[i for i in range(100)] for _ in range(100)]\n",
    "rdd_queue = [ssc.sparkContext.parallelize(item) for item in input_data]\n",
    "\n",
    "stream = ssc.queueStream(rdd_queue).transform(lambda rdd: rdd.filter(lambda x: x % 2 == 0))\n",
    "stream.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:24:48\n",
      "-------------------------------------------\n",
      "('y', 1)\n",
      "('u', 1)\n",
      "('n', 1)\n",
      "('q', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:24:49\n",
      "-------------------------------------------\n",
      "('y', 1)\n",
      "('u', 1)\n",
      "('n', 1)\n",
      "('q', 1)\n",
      "('q', 1)\n",
      "('j', 1)\n",
      "('b', 1)\n",
      "('l', 1)\n",
      "('o', 1)\n",
      "('x', 1)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:24:50\n",
      "-------------------------------------------\n",
      "('y', 1)\n",
      "('u', 1)\n",
      "('n', 1)\n",
      "('q', 1)\n",
      "('q', 1)\n",
      "('j', 1)\n",
      "('b', 1)\n",
      "('l', 1)\n",
      "('o', 1)\n",
      "('x', 1)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:24:55\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:24:56\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:24:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:24:58\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:24:59\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:25:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:25:01\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:25:02\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from time import sleep\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint('/tmp')\n",
    "\n",
    "lines = ssc.socketTextStream('0.0.0.0', 301)\n",
    "words = lines.flatMap(lambda s: s.split(' '))\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "\n",
    "stream = pairs.window(5, 1)\n",
    "stream.pprint()\n",
    "\n",
    "ssc.start()\n",
    "sleep(3)\n",
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count by window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:25:04\n",
      "-------------------------------------------\n",
      "4\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:25:05\n",
      "-------------------------------------------\n",
      "14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from time import sleep\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint('/tmp')\n",
    "\n",
    "lines = ssc.socketTextStream('0.0.0.0', 301)\n",
    "words = lines.flatMap(lambda s: s.split(' '))\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "\n",
    "stream = pairs.countByWindow(5, 1)\n",
    "stream.pprint()\n",
    "\n",
    "ssc.start()\n",
    "sleep(3)\n",
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce by window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:25:20\n",
      "-------------------------------------------\n",
      "('c', 1, 'v', 1, 'w', 1, 'c', 1, 'y', 1, 'r', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:25:21\n",
      "-------------------------------------------\n",
      "('c', 1, 'v', 1, 'w', 1, 'c', 1, 'y', 1, 'r', 1, 'c', 1, 'n', 1, 'q', 1, 'j', 1, 'p', 1, 'v', 1, 'y', 1, 'a', 1, 'm', 1, 'w', 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from time import sleep\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint('/tmp')\n",
    "\n",
    "lines = ssc.socketTextStream('0.0.0.0', 301)\n",
    "words = lines.flatMap(lambda s: s.split(' '))\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "\n",
    "stream = pairs.reduceByWindow(lambda a, b: a + b, lambda a, b: a - b, 5, 1)\n",
    "stream.pprint()\n",
    "\n",
    "ssc.start()\n",
    "sleep(3)\n",
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce by key and window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:25:36\n",
      "-------------------------------------------\n",
      "('81', 1)\n",
      "('67', 1)\n",
      "('55', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:25:37\n",
      "-------------------------------------------\n",
      "('4', 1)\n",
      "('10', 1)\n",
      "('81', 1)\n",
      "('67', 2)\n",
      "('62', 1)\n",
      "('49', 1)\n",
      "('96', 1)\n",
      "('9', 1)\n",
      "('55', 1)\n",
      "('5', 1)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:25:38\n",
      "-------------------------------------------\n",
      "('4', 1)\n",
      "('10', 1)\n",
      "('50', 1)\n",
      "('82', 1)\n",
      "('57', 1)\n",
      "('26', 1)\n",
      "('81', 1)\n",
      "('67', 3)\n",
      "('62', 1)\n",
      "('49', 1)\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from time import sleep\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint('/tmp')\n",
    "\n",
    "lines = ssc.socketTextStream('0.0.0.0', 300)\n",
    "words = lines.flatMap(lambda s: s.split(' '))\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "\n",
    "stream = pairs.reduceByKeyAndWindow(lambda a, b: a + b, lambda a, b: a - b, 5, 1)\n",
    "stream.pprint()\n",
    "\n",
    "ssc.start()\n",
    "sleep(3)\n",
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count by value and window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:25:52\n",
      "-------------------------------------------\n",
      "(('10', 1), 1)\n",
      "(('64', 1), 1)\n",
      "(('69', 1), 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:25:53\n",
      "-------------------------------------------\n",
      "(('6', 1), 1)\n",
      "(('80', 1), 1)\n",
      "(('31', 1), 1)\n",
      "(('81', 1), 1)\n",
      "(('25', 1), 1)\n",
      "(('30', 1), 1)\n",
      "(('10', 1), 1)\n",
      "(('64', 1), 1)\n",
      "(('53', 1), 1)\n",
      "(('16', 1), 1)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:25:54\n",
      "-------------------------------------------\n",
      "(('6', 1), 1)\n",
      "(('80', 1), 1)\n",
      "(('31', 1), 1)\n",
      "(('81', 1), 1)\n",
      "(('25', 1), 1)\n",
      "(('30', 1), 1)\n",
      "(('49', 1), 1)\n",
      "(('18', 1), 1)\n",
      "(('62', 1), 1)\n",
      "(('10', 1), 1)\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from time import sleep\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint('/tmp')\n",
    "\n",
    "lines = ssc.socketTextStream('0.0.0.0', 300)\n",
    "words = lines.flatMap(lambda s: s.split(' '))\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "\n",
    "stream = pairs.countByValueAndWindow(5, 1)\n",
    "stream.pprint()\n",
    "\n",
    "ssc.start()\n",
    "sleep(3)\n",
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-10-31 16:26:08\n",
      "-------------------------------------------\n",
      "(('42', 1), 1)\n",
      "(('30', 1), 1)\n",
      "(('57', 1), 1)\n",
      "(('38', 1), 1)\n",
      "(('93', 1), 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from time import sleep\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint('/tmp')\n",
    "\n",
    "lines = ssc.socketTextStream('0.0.0.0', 300)\n",
    "words = lines.flatMap(lambda s: s.split(' '))\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "\n",
    "stream = pairs.countByValueAndWindow(5, 1)\n",
    "stream.pprint()\n",
    "stream.saveAsTextFiles('dstream/data', 'txt')\n",
    "\n",
    "ssc.start()\n",
    "sleep(3)\n",
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
