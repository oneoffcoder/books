{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.transforms import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import copy\n",
    "\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    # img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "def get_dataloaders():\n",
    "    def clean_up(path):\n",
    "        import os\n",
    "        from shutil import rmtree\n",
    "        \n",
    "        ipynb_checkpoints = f'{path}/.ipynb_checkpoints'\n",
    "        if os.path.exists(ipynb_checkpoints):\n",
    "            rmtree(ipynb_checkpoints)\n",
    "            \n",
    "    def get_dataloader(phase):\n",
    "        path = f'./shapes/{phase}'\n",
    "        clean_up(path)\n",
    "        \n",
    "        if phase in ['valid']:\n",
    "            transform = transforms.Compose([\n",
    "                Resize(224), \n",
    "                ToTensor()])\n",
    "        else:\n",
    "            transform = transforms.Compose([\n",
    "                Resize(224), \n",
    "                RandomAffine(degrees=(30, 50), shear=5),\n",
    "                ToTensor()])\n",
    "            \n",
    "        image_folder = datasets.ImageFolder(path, transform=transform)\n",
    "        # print(path, image_folder.classes)\n",
    "        return DataLoader(image_folder, batch_size=4, shuffle=True, num_workers=4)\n",
    "    \n",
    "    return {phase: get_dataloader(phase) for phase in ['train', 'test', 'valid']}\n",
    "\n",
    "np.random.seed(37)\n",
    "torch.manual_seed(37)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "pretrained=True\n",
    "num_classes = 3\n",
    "writer = SummaryWriter('/root/tensorboard/shape_experiment_1')\n",
    "\n",
    "dataloaders = get_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_iter = iter(dataloaders['train'])\n",
    "images, labels = data_iter.next()\n",
    "\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "matplotlib_imshow(img_grid, one_channel=False)\n",
    "\n",
    "writer.add_image('four_shape_images', img_grid)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 47.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=pretrained)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(model, images.to(device))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataloader, device):\n",
    "    imgs = []\n",
    "    lbls = []\n",
    "    \n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        imgs.append(inputs)\n",
    "        lbls.append(labels)\n",
    "    \n",
    "    imgs = torch.cat(imgs)\n",
    "    lbls = torch.cat(lbls)\n",
    "    \n",
    "    return imgs, lbls\n",
    "    \n",
    "def select_n_random(data, labels, n=100):\n",
    "    assert len(data) == len(labels)\n",
    "\n",
    "    perm = torch.randperm(len(data))\n",
    "    return data[perm][:n], labels[perm][:n]\n",
    "\n",
    "images, labels = get_data(dataloaders['train'], device)\n",
    "images, labels = select_n_random(images, labels)\n",
    "\n",
    "idx_to_class = {v: k for k, v in dataloaders['train'].dataset.class_to_idx.items()}\n",
    "class_labels = [idx_to_class[lab] for lab in labels.cpu().detach().numpy()] \n",
    "\n",
    "writer.add_embedding(images.mean(dim=1).view(-1, 224 * 224), metadata=class_labels, label_img=images, global_step=1)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def images_to_probs(net, images):\n",
    "    output = net(images)\n",
    "    _, preds_tensor = torch.max(output, 1)\n",
    "    preds = np.squeeze(preds_tensor.cpu().detach().numpy())\n",
    "    return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]\n",
    "\n",
    "\n",
    "def plot_classes_preds(net, images, labels, classes):\n",
    "    preds, probs = images_to_probs(net, images)\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 48))\n",
    "    \n",
    "    n_images = images.shape[0]\n",
    "    \n",
    "    for idx in np.arange(n_images):\n",
    "        ax = fig.add_subplot(1, n_images, idx+1, xticks=[], yticks=[])\n",
    "        matplotlib_imshow(images[idx].cpu().detach(), one_channel=False)\n",
    "        \n",
    "        clazz_pred = classes[preds[idx]]\n",
    "        clazz_true = classes[labels[idx].cpu().detach().numpy().item()]\n",
    "        prob_pred = probs[idx] * 100.0\n",
    "        color = 'green' if preds[idx]==labels[idx].item() else 'red'\n",
    "        \n",
    "        ax.set_title(f'pred={clazz_pred}, {prob_pred:.2f}\\ntrue={clazz_true}', color=color)\n",
    "        \n",
    "    return fig\n",
    "\n",
    "def train(dataloaders, model, criterion, optimizer, scheduler, classes, device, writer, num_epochs=20):\n",
    "    def format_start_stop(start, stop):\n",
    "        elapsed = stop - start\n",
    "        return f'{elapsed//60:.0f}m {elapsed%50:.0f}s'\n",
    "    \n",
    "    best_model_weights = copy.deepcopy(model.state_dict())\n",
    "    best_acc = -1.0\n",
    "    \n",
    "    loop_start = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = 0.0, 0.0\n",
    "        test_loss, test_acc = 0.0, 0.0\n",
    "        train_start, test_start = 0.0, 0.0\n",
    "        train_stop, test_stop = 0.0, 0.0\n",
    "        \n",
    "        for i, phase in enumerate(['train', 'test']):\n",
    "            if phase == 'train':\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                model.train()\n",
    "                train_start = time.time()\n",
    "            else:\n",
    "                model.eval()\n",
    "                test_start = time.time()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            n = 0\n",
    "            dataloader = dataloaders[phase]\n",
    "            \n",
    "            for inputs, labels in dataloader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                running_loss += loss.mean() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                n += len(labels)\n",
    "                \n",
    "            if phase == 'train':\n",
    "                writer.add_scalar('training loss', running_loss, epoch * len(dataloader) + i)\n",
    "\n",
    "                writer.add_figure('predictions vs. actuals',\n",
    "                        plot_classes_preds(model, inputs, labels, classes),\n",
    "                        global_step=epoch * len(dataloader) + i)\n",
    "\n",
    "            epoch_loss = running_loss / float(n)\n",
    "            epoch_acc = running_corrects.double() / float(n)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                train_stop = time.time()\n",
    "                train_loss, train_acc = epoch_loss, epoch_acc\n",
    "            else:\n",
    "                test_stop = time.time()\n",
    "                test_loss, test_acc = epoch_loss, epoch_acc\n",
    "                \n",
    "                if epoch_acc > best_acc:\n",
    "                    best_model_weights = copy.deepcopy(model.state_dict())\n",
    "                    best_acc = epoch_acc                \n",
    "        \n",
    "        train_time = format_start_stop(train_start, train_stop)\n",
    "        test_time = format_start_stop(test_start, test_stop)\n",
    "        \n",
    "        train_metrics = f'TRAIN: {train_loss:.4f}, {train_acc:.4f}, {train_time}'\n",
    "        test_metrics = f'TEST: {test_loss:.4f}, {test_acc:.4f}, {test_time}'\n",
    "        print(f'epoch {str(epoch + 1).zfill(2)}/{str(num_epochs).zfill(2)} | {train_metrics} | {test_metrics}')\n",
    "    \n",
    "    loop_stop = time.time()\n",
    "    loop_time = format_start_stop(loop_start, loop_stop)\n",
    "    print(f'completed learning in {loop_time}, best accuracy {best_acc:.4f}')\n",
    "    \n",
    "    model.load_state_dict(best_model_weights)\n",
    "    writer.close()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 01/20 | TRAIN: 1.0803, 0.4667, 0m 1s | TEST: 0.0827, 1.0000, 0m 0s\n",
      "epoch 02/20 | TRAIN: 0.5203, 0.8333, 0m 1s | TEST: 0.0882, 1.0000, 0m 0s\n",
      "epoch 03/20 | TRAIN: 0.7947, 0.8000, 0m 1s | TEST: 0.1361, 0.9333, 0m 0s\n",
      "epoch 04/20 | TRAIN: 0.1635, 0.9333, 0m 1s | TEST: 0.0064, 1.0000, 0m 0s\n",
      "epoch 05/20 | TRAIN: 0.1467, 0.9333, 0m 1s | TEST: 0.0036, 1.0000, 0m 0s\n",
      "epoch 06/20 | TRAIN: 0.1230, 0.9333, 0m 1s | TEST: 0.0016, 1.0000, 0m 0s\n",
      "epoch 07/20 | TRAIN: 0.3416, 0.9000, 0m 1s | TEST: 0.0015, 1.0000, 0m 0s\n",
      "epoch 08/20 | TRAIN: 0.0392, 1.0000, 0m 1s | TEST: 0.0029, 1.0000, 0m 0s\n",
      "epoch 09/20 | TRAIN: 0.3981, 0.8000, 0m 1s | TEST: 0.0006, 1.0000, 0m 0s\n",
      "epoch 10/20 | TRAIN: 0.1058, 0.9667, 0m 1s | TEST: 0.0010, 1.0000, 0m 0s\n",
      "epoch 11/20 | TRAIN: 0.3351, 0.8667, 0m 1s | TEST: 0.0007, 1.0000, 0m 0s\n",
      "epoch 12/20 | TRAIN: 0.0615, 1.0000, 0m 1s | TEST: 0.0011, 1.0000, 0m 0s\n",
      "epoch 13/20 | TRAIN: 0.0738, 0.9667, 0m 1s | TEST: 0.0367, 1.0000, 0m 0s\n",
      "epoch 14/20 | TRAIN: 0.2468, 0.9333, 0m 1s | TEST: 0.0108, 1.0000, 0m 0s\n",
      "epoch 15/20 | TRAIN: 0.2451, 0.9333, 0m 1s | TEST: 0.0019, 1.0000, 0m 0s\n",
      "epoch 16/20 | TRAIN: 0.1233, 0.9333, 0m 1s | TEST: 0.0038, 1.0000, 0m 0s\n",
      "epoch 17/20 | TRAIN: 0.3774, 0.8333, 0m 1s | TEST: 0.0005, 1.0000, 0m 0s\n",
      "epoch 18/20 | TRAIN: 0.1251, 0.9333, 0m 1s | TEST: 0.0039, 1.0000, 0m 0s\n",
      "epoch 19/20 | TRAIN: 0.2142, 0.9333, 0m 1s | TEST: 0.0005, 1.0000, 0m 0s\n",
      "epoch 20/20 | TRAIN: 0.2402, 0.8667, 0m 1s | TEST: 0.0014, 1.0000, 0m 0s\n",
      "completed learning in 0m 20s, best accuracy 1.0000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "model = models.resnet18(pretrained=pretrained)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Rprop(model.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1)\n",
    "\n",
    "model = train(dataloaders, model, criterion, optimizer, scheduler, idx_to_class, device, writer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add precision-recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_probs = []\n",
    "class_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in dataloaders['valid']:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        output = model(images)\n",
    "        class_probs_batch = [F.softmax(el, dim=0) for el in output]\n",
    "        _, class_preds_batch = torch.max(output, 1)\n",
    "\n",
    "        class_probs.append(class_probs_batch)\n",
    "        class_preds.append(class_preds_batch)\n",
    "\n",
    "test_probs = torch.cat([torch.stack(batch) for batch in class_probs])\n",
    "test_preds = torch.cat(class_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9846e-01, 1.1767e-03, 3.6261e-04],\n",
       "        [9.9739e-01, 1.9887e-03, 6.2254e-04],\n",
       "        [9.9639e-01, 2.3840e-03, 1.2309e-03],\n",
       "        [9.2196e-05, 9.9922e-01, 6.8980e-04],\n",
       "        [8.1074e-03, 2.1422e-01, 7.7767e-01],\n",
       "        [4.3053e-04, 3.0992e-02, 9.6858e-01],\n",
       "        [7.7913e-10, 9.9995e-01, 5.1609e-05],\n",
       "        [9.9911e-01, 6.2751e-04, 2.5947e-04],\n",
       "        [1.1694e-10, 1.0000e+00, 5.2564e-08],\n",
       "        [9.9906e-12, 1.0000e+00, 2.7644e-08],\n",
       "        [1.0453e-08, 9.9999e-01, 1.1971e-05],\n",
       "        [2.8406e-05, 4.6754e-02, 9.5322e-01],\n",
       "        [7.9143e-13, 1.0000e+00, 3.6505e-09],\n",
       "        [9.9958e-01, 2.8400e-04, 1.3418e-04],\n",
       "        [3.1156e-03, 1.6711e-01, 8.2978e-01]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 1, 2, 2, 1, 0, 1, 1, 1, 2, 1, 0, 2], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pr_curve_tensorboard(class_index, test_probs, test_preds, idx_to_class, global_step=0):\n",
    "    tensorboard_preds = test_preds == class_index\n",
    "    tensorboard_probs = test_probs[:, class_index]\n",
    "\n",
    "    writer.add_pr_curve(idx_to_class[class_index],\n",
    "                        tensorboard_preds,\n",
    "                        tensorboard_probs,\n",
    "                        global_step=global_step)\n",
    "    writer.close()\n",
    "    \n",
    "n_classes = len(dataloaders['valid'].dataset.classes)\n",
    "\n",
    "for i in range(n_classes):\n",
    "    add_pr_curve_tensorboard(i, test_probs, test_preds, idx_to_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
