{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2b2ea2d-f7b8-470b-b18e-8c946908b7d4",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent for Online Learning\n",
    "\n",
    "Let's take a look at how to use Stochastic Gradient Descent (SGD) for online learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d4f606-e2c8-494c-820d-854d0d7ecc2e",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We will simulate data for a regression problem. The data is simulated as follows.\n",
    "\n",
    "- $X_1 \\sim \\mathcal{N}(2, 1)$\n",
    "- $X_2 \\sim \\mathcal{N}(8.8, 1)$\n",
    "- $Y \\sim \\mathcal{N}(5.0 + 2 X_1 - 1.5 X_2, 1)$\n",
    "\n",
    "Note how we create 3 `Xy` samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fefb78fd-777f-4ab2-95e4-57668ef272d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "np.random.seed(37)\n",
    "num_samples = 100\n",
    "\n",
    "def get_Xy():\n",
    "    x1 = 2.0 + np.random.standard_normal(num_samples)\n",
    "    x2 = 8.8 + np.random.standard_normal(num_samples)\n",
    "\n",
    "    y = 5.0 + 2.0 * x1 - 1.5 * x2 + np.random.standard_normal(num_samples)\n",
    "\n",
    "    X = np.column_stack((x1, x2))\n",
    "    return X, y\n",
    "\n",
    "data = [get_Xy(), get_Xy(), get_Xy()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2a1b73-2f53-430e-8698-97384c1b074e",
   "metadata": {},
   "source": [
    "## Scikit-Learn, Linear Regression\n",
    "\n",
    "We can use the following models to apply regression to the data.\n",
    "\n",
    "- [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "- [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso)\n",
    "- [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge)\n",
    "\n",
    "Clearly, the approaches produce different intercepts and weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da0adac7-1922-41b0-b187-3776606183a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.6219320466614855 [ 2.16262535 -1.50176798]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X, y = data[0]\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "print(lr.intercept_, lr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75eb1fb0-bf31-4620-8a58-f851fcaba3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.02407792798139 [ 1.11832673 -0.3863843 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "X, y = data[0]\n",
    "\n",
    "lr = Lasso()\n",
    "lr.fit(X, y)\n",
    "print(lr.intercept_, lr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4a38629-20c4-462d-b919-c04bf1b40571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.516849362852162 [ 2.14079312 -1.48468401]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "X, y = data[0]\n",
    "\n",
    "lr = Ridge()\n",
    "lr.fit(X, y)\n",
    "print(lr.intercept_, lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f36275-45db-4c0c-bdf9-35f4697c04da",
   "metadata": {},
   "source": [
    "## Scikit-Learn, SGD Regression\n",
    "\n",
    "If we needed to do online learning using Scikit-Learn, we can use [SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html). Pay attention to the `partial_fit()` method, which executes one `epoch` of training on the data; an epoch is just one iteration of weight learning. In this example, we learn the coefficients/weights with 20,000 epochs on the first sampled `Xy` data, followed by another 20,000 epochs on the second sampled `Xy` data.\n",
    "\n",
    "In the first round of training, the coefficients are quite off from the true ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b80cd82-6c46-4945-b2ba-5d1dbdfe8476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.621363227569006 [ 2.16332485 -1.49836651]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "X, y = data[0]\n",
    "sgd = SGDRegressor()\n",
    "\n",
    "for _ in range(20_000):\n",
    "    sgd.partial_fit(X, y)\n",
    "\n",
    "print(sgd.intercept_[0], sgd.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647c893a-89dd-48ab-9b2a-595820761e46",
   "metadata": {},
   "source": [
    "With online learning, the coefficients are now very close to the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a7eab0a-c346-4bcd-9d87-0f03b55de54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.106512059932266 [ 1.97471252 -1.52916083]\n"
     ]
    }
   ],
   "source": [
    "X, y = data[1]\n",
    "\n",
    "for _ in range(20_000):\n",
    "    sgd.partial_fit(X, y)\n",
    "\n",
    "print(sgd.intercept_[0], sgd.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77d2406-f463-4aee-a32d-c30b48bc47ab",
   "metadata": {},
   "source": [
    "## SGD\n",
    "\n",
    "We can implement SGD by hand. This function `step()` will evaluate the gradients of the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38047f5b-7124-4bfb-b126-1ad82190feff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(X, y, b, w, N, alpha=0.005, freeze={}):\n",
    "    def get_w_grad(c, y_diff):\n",
    "        if c in freeze:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return -2 * (X[:,c] * y_diff).mean()\n",
    "    y_pred = b + X.dot(w)\n",
    "    y_diff = y - y_pred\n",
    "    \n",
    "    b_grad = -2 * y_diff.mean()\n",
    "    w_grad = np.array([get_w_grad(c, y_diff) for c in range(X.shape[1])])\n",
    "\n",
    "    b_new = b - alpha * b_grad\n",
    "    w_new = w - alpha * w_grad\n",
    "    \n",
    "    return b_new, w_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759990fb-406b-4989-881d-a749e20395a1",
   "metadata": {},
   "source": [
    "### Gradient descent, batch\n",
    "\n",
    "With Batch Gradient Descent `BGD`, for each epoch, we feed the complete training data to evaluate the gradients of the weights. We specify 30,000 epochs for batch training (BGD). The weights learned are still not close to the true values. \n",
    "\n",
    "Note that we initialize the weights to all zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c1ccb26-5900-4fc4-bc8b-8d0eb557d9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: b = -0.04113658111994803, w = [-0.0634133  -0.37421837]\n",
      "1000: b = 0.4597592414630508, w = [ 2.22724475 -1.04881832]\n",
      "2000: b = 0.9316684307305253, w = [ 2.22000329 -1.10019403]\n",
      "3000: b = 1.3500730784489052, w = [ 2.21349774 -1.14572477]\n",
      "4000: b = 1.721038717105341, w = [ 2.20772979 -1.18609319]\n",
      "5000: b = 2.049944014408199, w = [ 2.20261582 -1.22188462]\n",
      "6000: b = 2.341557801026509, w = [ 2.19808167 -1.25361799]\n",
      "7000: b = 2.6001082143011445, w = [ 2.19406161 -1.28175341]\n",
      "8000: b = 2.8293440024292913, w = [ 2.19049735 -1.30669881]\n",
      "9000: b = 3.032588877938021, w = [ 2.1873372  -1.32881589]\n",
      "10000: b = 3.2127897085232853, w = [ 2.18453535 -1.34842532]\n",
      "11000: b = 3.372559243978179, w = [ 2.18205118 -1.36581142]\n",
      "12000: b = 3.514213998712492, w = [ 2.17984867 -1.38122627]\n",
      "13000: b = 3.6398078391258446, w = [ 2.17789587 -1.39489337]\n",
      "14000: b = 3.7511617628212286, w = [ 2.17616449 -1.40701089]\n",
      "15000: b = 3.849890301430676, w = [ 2.17462941 -1.41775451]\n",
      "16000: b = 3.9374249298701827, w = [ 2.17326838 -1.42728002]\n",
      "17000: b = 4.015034821437141, w = [ 2.17206167 -1.43572551]\n",
      "18000: b = 4.083845249680377, w = [ 2.17099177 -1.44321346]\n",
      "19000: b = 4.144853903853434, w = [ 2.17004318 -1.44985241]\n",
      "20000: b = 4.198945354510451, w = [ 2.16920214 -1.45573863]\n",
      "21000: b = 4.246903878982752, w = [ 2.16845646 -1.46095747]\n",
      "22000: b = 4.289424832694, w = [ 2.16779532 -1.4655846 ]\n",
      "23000: b = 4.327124731187709, w = [ 2.16720915 -1.46968709]\n",
      "24000: b = 4.360550189047555, w = [ 2.16668943 -1.47332445]\n",
      "25000: b = 4.3901858453166644, w = [ 2.16622865 -1.47654939]\n",
      "26000: b = 4.416461390327421, w = [ 2.1658201 -1.4794087]\n",
      "27000: b = 4.439757795824325, w = [ 2.16545788 -1.48194381]\n",
      "28000: b = 4.460412838711318, w = [ 2.16513672 -1.48419149]\n",
      "29000: b = 4.478725998512731, w = [ 2.16485198 -1.48618432]\n",
      "final: b = 4.494947519211368, w = [ 2.16459976 -1.48794954]\n"
     ]
    }
   ],
   "source": [
    "X, y = data[0]\n",
    "\n",
    "b = 0.0\n",
    "w = np.zeros(X.shape[1])\n",
    "alpha = 0.01\n",
    "N = X.shape[0]\n",
    "\n",
    "for i in range(30_000):\n",
    "    b_new, w_new = step(X, y, b, w, alpha)\n",
    "    b = b_new\n",
    "    w = w_new\n",
    "    if i % 1000 == 0:\n",
    "        print(f'{i}: b = {b_new}, w = {w_new}')\n",
    "\n",
    "print(f'final: b = {b}, w = {w}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f672bd3-03ff-4aa6-999a-ea5b3475701c",
   "metadata": {},
   "source": [
    "### Gradient descent, stochastic\n",
    "\n",
    "For SGD, during each epoch, we feed one random sample at a time to evaluate the gradients of the weights. This time, we specify 10,000 epochs. Generally speaking, SGD is preferred over BGD since the former converges faster.\n",
    "\n",
    "Note that we initialize the weights to all zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d836af0-b8a8-45ea-b6bb-93184c378618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: b = 0.1302635564715835, w = [ 2.40138751 -0.24934505]\n",
      "1000: b = 4.666510646800165, w = [ 2.19419193 -1.18461641]\n",
      "2000: b = 5.057244621464196, w = [ 2.21689666 -1.40694691]\n",
      "3000: b = 4.479227120983244, w = [ 2.01991825 -1.24664366]\n",
      "4000: b = 4.56540770809679, w = [ 2.52343085 -1.30036674]\n",
      "5000: b = 4.419414341369592, w = [ 1.80293121 -1.4658816 ]\n",
      "6000: b = 4.440845181302684, w = [ 2.26876065 -1.07881225]\n",
      "7000: b = 5.027389277961371, w = [ 2.27691608 -1.22757039]\n",
      "8000: b = 4.637299491755886, w = [ 1.87026482 -1.2841124 ]\n",
      "9000: b = 4.44058042372063, w = [ 2.28106253 -1.63058405]\n",
      "final: b = 4.416424432571243, w = [ 2.33365453 -1.48993813]\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "X, y = data[0]\n",
    "\n",
    "b = 0.0\n",
    "w = np.zeros(X.shape[1])\n",
    "alpha = 0.01\n",
    "N = X.shape[0]\n",
    "\n",
    "for i in range(10_000):\n",
    "    indices = list(range(X.shape[0]))\n",
    "    shuffle(indices)\n",
    "\n",
    "    for r in indices:\n",
    "        b_new, w_new = step(X[r,:].reshape(1,-1), y[r], b, w, N, alpha=alpha)\n",
    "        b = b_new\n",
    "        w = w_new\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(f'{i}: b = {b_new}, w = {w_new}')\n",
    "\n",
    "print(f'final: b = {b}, w = {w}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758a2566-c7fc-493a-8ebb-d0285af4f1ff",
   "metadata": {},
   "source": [
    "### Gradient descent, stochastic, online\n",
    "\n",
    "Now, we keep the learned weights from before and seed the algorithm with these weights (we do not initialize or guess the weights as all zero). However, we do change the data set (as if new data are coming through and we need to re-learn the parameters/weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1696376c-69c5-4ec8-bea7-2e0a66323df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: b = 4.411319583679032, w = [ 1.85441694 -1.28729906]\n",
      "final: b = 4.411319583679032, w = [ 1.85441694 -1.28729906]\n"
     ]
    }
   ],
   "source": [
    "X, y = data[1]\n",
    "N = X.shape[0]\n",
    "\n",
    "for i in range(1):\n",
    "    indices = list(range(X.shape[0]))\n",
    "    shuffle(indices)\n",
    "\n",
    "    for r in indices:\n",
    "        b_new, w_new = step(X[r,:].reshape(1,-1), y[r], b, w, N, alpha=alpha)\n",
    "        b = b_new\n",
    "        w = w_new\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(f'{i}: b = {b_new}, w = {w_new}')\n",
    "\n",
    "print(f'final: b = {b}, w = {w}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
